{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data -1 | Link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Link KNN with Turnout\n",
    "\n",
    "A list of elections in North Carolina https://en.wikipedia.org/wiki/Elections_in_North_Carolina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NC_2010\n"
     ]
    }
   ],
   "source": [
    "from datalink import *\n",
    "\n",
    "with open(path_0 + 'turnout_history_20190101.pkl','rb') as f: \n",
    "    history = pickle.load(f)\n",
    "elections = history.election_desc.unique()\n",
    "\n",
    "# 'MUNCIPAL', 'MUNI'\n",
    "# 'EDUCATION'\n",
    "# 'ALCHOHOL', 'BEVERAGE'\n",
    "\n",
    "election_types = ['GENERAL', 'PRIMARY', 'MUNICIPAL',]# 'SCHOOL', 'REFERENDUM', 'MUNI AND SANI DISTRICT', 'ABC']\n",
    "election_types = {election_type: [e for e in elections if election_type in e] for election_type in election_types}\n",
    "\n",
    "\n",
    "files = set([x.split('_chunk_')[0] for x in os.listdir(path_1) if x.split('.')[-1] == 'pkl'])\n",
    "for file in sorted(files):\n",
    "    \n",
    "    state, year = file.split('_')\n",
    "    print(file)\n",
    "    \n",
    "    last_election_type = [e for e in election_types][-1]\n",
    "    chunk_file_names = [x for x in os.listdir(path_1) if file in x]\n",
    "    chunk_paths = [x.strip(file) for x in chunk_file_names]\n",
    "    max_chunk = max([x for x in np.arange(0,30) if str(x) in '_'.join(chunk_paths)])\n",
    "    \n",
    "    if file + '_' + last_election_type + '_chunk_' + str(max_chunk) + '.pkl' not in os.listdir(path_link):\n",
    "        \n",
    "        knn_voters = pd.read_pickle(path_4+'knn_' + year + '_' + str(k_max) + 'k.pkl')\n",
    "        \n",
    "        # this is crashing the kernal!^\n",
    "        # I need to rewrite knn for chunks\n",
    "        \n",
    "        knn_idu_list = knn_voters.idu.unique()\n",
    "        print('  KNN:',len(knn_voters))\n",
    "\n",
    "        for election_type in election_types:\n",
    "            if year + '_' + election_type + '.pkl' not in os.listdir(path_link):\n",
    "                print('  ',election_type)\n",
    "                keep_election_cols = ['voted_' + e for e in election_types[election_type] if (year in e) or (str(int(year) - 1) in e)]\n",
    "                \n",
    "                for chunk_name in sorted(chunk_file_names):\n",
    "                    knn_chunk_name = chunk_name.replace('chunk','knn_' + election_type + '_chunk')\n",
    "\n",
    "                    chunk = pd.read_pickle(path_1 + chunk_name)\n",
    "                    chunk = chunk[['idu', 'D', 'R', 'O'] + keep_election_cols]\n",
    "                    chunk = knn_voters.merge(chunk, on='idu', how='inner')\n",
    "                    \n",
    "                    chunk.to_pickle(path_link + 'chunks/' + knn_chunk_name)\n",
    "                    print('  ||',len(chunk),'voters')\n",
    "                    \n",
    "            else:\n",
    "                print('  ||',election_type,'Done')\n",
    "        del knn_voters\n",
    "        \n",
    "    else:\n",
    "        print('  || Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ACS and Squares\n",
    "\n",
    "Add ACS interpolated data to knn samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 'NC'\n",
    "linkpath = '../voters/data5_link/'+state+'/'\n",
    "\n",
    "with open('../voters/data4_knn/'+state+'/key_id_subsamples.pkl','rb') as f: key_id_subsamples = pickle.load(f)\n",
    "keys = [key for key in key_id_subsamples]\n",
    "datadict = {}\n",
    "link_ids = []\n",
    "for key in keys:\n",
    "    if key+'_plus.csv' in os.listdir(linkpath):\n",
    "        datadict[key] = pd.read_csv(linkpath+key+'_plus.csv', low_memory=False)\n",
    "    else:\n",
    "        datadict[key] = pd.read_csv(linkpath+key+'.csv', low_memory=False)\n",
    "    link_ids = link_ids + list(datadict[key].DOUBLE_ID.unique())\n",
    "link_ids = list(set(link_ids)) # List of subsample DOUBLE_IDS\n",
    "\n",
    "meterlist = [500]#,2000]\n",
    "for meters in meterlist:\n",
    "    datalist = []\n",
    "    for year in nc_yearlist:\n",
    "        with open('../voters/data3_neighborhoods/'+state+'/SNd_'+str(year)+'_'+str(meters)+'m.pkl','rb') as f: SNd = pickle.load(f)\n",
    "        finished = []\n",
    "        for ni in SNd:\n",
    "            n = SNd[ni]\n",
    "            n['YEAR'] = year\n",
    "            n['M_'+str(meters)+'m'] = sum((n.GENDER == 'M')*1)\n",
    "            n['F_'+str(meters)+'m'] = sum((n.GENDER == 'F')*1)\n",
    "            n['BLACK_'+str(meters)+'m'] = sum((n.RACE == 'BLACK or AFRICAN AMERICAN')*1)\n",
    "            n['WHITE_'+str(meters)+'m'] = sum((n.RACE == 'WHITE')*1)\n",
    "            n['OTHER_'+str(meters)+'m'] = sum((~n.RACE.isin(['BLACK or AFRICAN AMERICAN','WHITE']))*1)\n",
    "            n['D_'+str(meters)+'m'] = n.D.sum()\n",
    "            n['R_'+str(meters)+'m'] = n.R.sum()\n",
    "            n['O_'+str(meters)+'m'] = n.O.sum()\n",
    "            n['MEAN_AGE_'+str(meters)+'m'] = n.AGE.astype(float).mean()\n",
    "            print(year,round(len(finished)/len(SNd)*100,2),'%')\n",
    "            clear_output(wait=True)\n",
    "            finished.append(ni)\n",
    "        SNfinal = [SNd[i] for i in SNd]\n",
    "        SNfinal = pd.concat(SNfinal,ignore_index=True, sort=False)\n",
    "        SNfinal['DOUBLE_ID'] = SNfinal['ID_NUM'] + SNfinal['ID_NUM_a']\n",
    "        SNfinal = SNfinal[SNfinal.DOUBLE_ID.isin(link_ids)]\n",
    "        datalist.append(SNfinal)\n",
    "\n",
    "    linkdata = pd.concat(datalist)\n",
    "    linkdata = linkdata.sort_values(by=['DOUBLE_ID','YEAR'])\n",
    "    \n",
    "    keys = ['move','switch','stay','move_switch']\n",
    "    for key in keys:\n",
    "        key_linkdata = linkdata[linkdata.DOUBLE_ID.isin(datadict[key].DOUBLE_ID.unique())]\n",
    "        key_linkdata['DOUBLE_ID_YEAR'] = key_linkdata['DOUBLE_ID'] + '_' + key_linkdata['YEAR'].astype(str)\n",
    "        key_linkdata_droplist = ['ID_NUM','CITY','STATE','ZIP_CODE','ID_NUM_a','GENDER','AGE','RACE','sn_i','lat','lon','YEAR','D','R','O']\n",
    "        key_linkdata_keeplist = [col for col in key_linkdata.columns.values if col not in key_linkdata_droplist]\n",
    "        key_linkdata = key_linkdata[key_linkdata_keeplist]\n",
    "\n",
    "        datadict[key]['DOUBLE_ID_YEAR'] = datadict[key]['DOUBLE_ID'] + '_' + datadict[key]['YEAR'].astype(str)\n",
    "\n",
    "        merge = datadict[key].merge(key_linkdata, left_on='DOUBLE_ID_YEAR', right_on='DOUBLE_ID_YEAR', how='outer')\n",
    "        rename_dict = {x:x.replace('_x','') for x in squares_keep_cols if '_x' in x}\n",
    "        merge = merge.rename(columns=rename_dict)\n",
    "\n",
    "        merge.to_csv('data5_link/NC/'+key+'_plus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#autor_variables = ['POP', 'WHITE', 'BLACK', 'VOTING_POP', 'LABOR',\n",
    "#       'COLLEGE', 'COLLEGE_EMPLOYED', 'SOME_COLLEGE',\n",
    "#       'SOME_COLLEGE_EMPLOYED', 'NO_COLLEGE', 'NO_COLLEGE_EMPLOYED',\n",
    "#       'TRANTIME', 'COLLEGE_WAGE', 'SOME_COLLEGE_WAGE', 'NO_COLLEGE_WAGE']\n",
    "#for col in autor_variables:\n",
    "#    move[col+'_past'] = move[col].shift(1)\n",
    "#move = move[move['DOUBLE_ID'] == move['DOUBLE_ID'].shift(1)]\n",
    "#move.to_csv('data5_link/NC/'+keys[0]+'_plus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for col in ['DOUBLE_ID','ZIP_CODE','sn_i','lat','lon','D','R','O']:#+return_k_col_names(state):\n",
    "#    colshiftname = col+'_past'\n",
    "#    linkdata[colshiftname] = linkdata[col].shift(1)\n",
    "#linkdata = linkdata[linkdata['DOUBLE_ID'] == linkdata['DOUBLE_ID'].shift(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date, time, datetime\n",
    "#date.today()\n",
    "datetime.now().strftime('%H')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
