{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data 2 | Geocode Voters\n",
    "\n",
    "### New Version \n",
    "\n",
    "The new version solves the problem of low geocoding success in some years and improves the linking rate. This is done by creating a list of all geocoding successes and merging this coder file 1:m with each voter file.\n",
    "\n",
    "1. Create a file of all addresses: addresses.\n",
    "2. Geocode as many as possible to create a file of geocoded address: geocoded. Since I've already geocoded every year, I can just use these data without needing to use the Census Bureau API.\n",
    "3. Link geocoded addresses with their geocoded variables: coder.\n",
    "4. Iterate over the ungeocoded addresses (unfinished). This is basically like running the old version on the ungeocoded file. \n",
    "5. Then merge each voter snapshot with coder 1:m.\n",
    "\n",
    "### Old Version\n",
    "\n",
    "I look at the cleaned files and check whether they're already geocoded. I geocode them using batch_geocode() if they're not geocoded.\n",
    "\n",
    "The function batch_geocode() takes the voter dataframe, then\n",
    "1. Drops unnecessary variables\n",
    "2. Creates batches\n",
    "3. Geocodes each batch\n",
    "4. Merges the batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" No need to rerun this. \"\"\"\n",
    "\n",
    "from data2_geocode import *\n",
    "\n",
    "\"\"\" Step 0 | Setup \"\"\"\n",
    "batch_size = 250\n",
    "address_cols = ['parsed', 'address', 'city', 'state', 'zipcode']\n",
    "\n",
    "\"\"\" Step 1 | Create Addresses \"\"\"\n",
    "\n",
    "if 'addresses.pkl' not in os.listdir(path_2):\n",
    "    addresses = pd.DataFrame(columns=address_cols)\n",
    "    addresses.to_pickle(path_2 + 'addresses.pkl')\n",
    "\n",
    "files = set([x.split('_chunk_')[0] for x in os.listdir(path_1) if 'pkl' in x])\n",
    "for file in sorted(files):\n",
    "    print(file)\n",
    "    \n",
    "    v = voters_from_chunks(path_1, file + '_', keep_cols=['address','city','state','zipcode'])\n",
    "    for col in v.columns.values:\n",
    "        v[col] = v[col].astype('str')\n",
    "    v['parsed'] = v['address'] + ', ' + v['city'] + ', ' + v['state'] + ', ' + v['zipcode']\n",
    "    v = v.drop_duplicates(subset=['parsed'])\n",
    "    \n",
    "    addresses = pd.read_pickle(path_2+'addresses.pkl')\n",
    "    addresses = pd.concat([addresses, v])\n",
    "    addresses = addresses.drop_duplicates(subset=['parsed'])\n",
    "    addresses.to_pickle(path_2 + 'addresses.pkl')\n",
    "    print('',len(addresses))\n",
    "    \n",
    "    del addresses, v\n",
    "    \n",
    "\"\"\" Step 2 | Create Geocoded \"\"\"\n",
    "\n",
    "geo_cols = ['matchtype', 'parsed', 'tigerlineid', 'side', 'statefp', 'countyfp', 'tract', 'block', 'lat', 'lon']\n",
    "\n",
    "if 'geocoded.pkl' in os.listdir(path_2):\n",
    "    geocoded = pd.DataFrame(columns=geo_cols)\n",
    "    geocoded.to_pickle(path_2 + 'geocoded.pkl')\n",
    "    \n",
    "\"\"\" Add Geocoded from Previous Files \"\"\"\n",
    "\n",
    "files__ = [x for x in os.listdir(path_2+'/archive/') if '_geo__.pkl' in x]\n",
    "files_ = [x for x in os.listdir(path_2+'/archive/') if '_geo_.pkl' in x]\n",
    "for file in sorted(files__ + files_):\n",
    "    print(file)\n",
    "    geocoded = pd.read_pickle(path_2+'geocoded.pkl')\n",
    "    geo = pd.read_pickle(path_2+'/archive/'+file)[geo_cols]\n",
    "    geo = geo[geo.matchtype.isin(['Exact', 'Non_Exact'])]\n",
    "    geo = geo.drop_duplicates(subset=['parsed'])\n",
    "    \n",
    "    geocoded = pd.concat([geocoded, geo])\n",
    "    geocoded = geocoded.drop_duplicates(subset=['parsed'])\n",
    "    geocoded.to_pickle(path_2 + 'geocoded.pkl')\n",
    "    print('',len(geocoded))\n",
    "    \n",
    "    del geo, geocoded\n",
    "    \n",
    "\"\"\" Step 3 | Create Coder and Uncoded \"\"\"\n",
    "\n",
    "from data2_geocode import *\n",
    "\n",
    "geocoded = pd.read_pickle(path_2+'geocoded.pkl').drop_duplicates(subset=['parsed'])\n",
    "addresses = pd.read_pickle(path_2+'addresses.pkl').drop_duplicates(subset=['parsed'])\n",
    "\n",
    "addresses.index = addresses.parsed\n",
    "geocoded.index = geocoded.parsed\n",
    "coder = pd.concat([addresses, geocoded], axis=1, join=\"inner\")\n",
    "coder = coder.drop('parsed', axis=1)\n",
    "coder.to_pickle(path_2 + 'coder.pkl')\n",
    "\n",
    "uncoded = addresses[~addresses.parsed.isin(geocoded.parsed.unique())]\n",
    "uncoded = uncoded.drop('parsed', axis=1)\n",
    "uncoded.to_pickle(path_2 + 'uncoded.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20051125\n",
      "  Done\n",
      "20061020\n",
      "  Done\n",
      "20070119\n",
      "  Done\n",
      "20090101\n",
      "  Merge 1:m\n",
      "   4631861\n",
      "20101102\n",
      "  Done\n",
      "20110101\n",
      "  Merge 1:m\n",
      "   4584142\n",
      "20121106\n",
      "  Done\n",
      "20130101\n",
      "  Done\n",
      "20141231\n",
      "  Merge 1:m\n",
      "   5568082\n",
      "20151103\n",
      "  Done\n",
      "20161108\n",
      "  Done\n",
      "20171107\n",
      "  Done\n",
      "20181106\n",
      "  Done\n",
      "20191105\n",
      "  Merge 1:m\n",
      "   5774976\n",
      "20201103\n",
      "  Merge 1:m\n",
      "   6217404\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Step 5 | Use Coder with Years \"\"\"\n",
    "\n",
    "from data2_geocode import *\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "coder = pd.read_pickle(path_2 + 'coder.pkl').reset_index()\n",
    "for col in ['address', 'city', 'state', 'zipcode']:\n",
    "    coder[col] = coder[col].astype('str').str.split().str.join(' ')\n",
    "coder['parsed'] = coder['address'] + ', ' + coder['city'] + ', ' + coder['state'] + ', ' + coder['zipcode']\n",
    "coder = coder.drop(['address', 'city', 'state', 'zipcode'], axis=1)\n",
    "\n",
    "files = list(set([x.split('_chunk_')[0] for x in os.listdir(path_1) if 'pkl' in x]))\n",
    "#done_files = [x.split('_')[0] for x in os.listdir(path_2) if '_geo.pkl' in x]\n",
    "#files = [x for x in files if x not in done_files]\n",
    "for file in sorted(files):\n",
    "    year = file\n",
    "    save_file = f'{file}_geo.pkl'\n",
    "    print(file)\n",
    "    \n",
    "    if save_file in os.listdir(path_2):\n",
    "        print('  Done')\n",
    "    \n",
    "    if save_file not in os.listdir(path_2):\n",
    "        print('  Merge 1:m')\n",
    "        \n",
    "        voters = voters_from_chunks(path_1, file, keep_cols=['idu','address','city','state','zipcode'])\n",
    "        for col in voters.columns.values:\n",
    "            voters[col] = voters[col].astype('str').str.split().str.join(' ')\n",
    "        voters['parsed'] = voters['address'] + ', ' + voters['city'] + ', ' + voters['state'] + ', ' + voters['zipcode']\n",
    "        \n",
    "        geodata = pd.merge(coder, voters, left_on='parsed', right_on='parsed', validate='1:m')\n",
    "        geodata.to_pickle(path_2 + save_file)\n",
    "        print('  ',len(geodata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 | Geocode the Ungeocoded in Addresses (unfinished)\n",
    "\n",
    "I still don't understand this error:\n",
    "\n",
    "InvalidIndexError: Reindexing only valid with uniquely valued Index objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step 4 | Run Batches on Uncoded\"\"\"\n",
    "\n",
    "from data2_geocode import *\n",
    "\n",
    "\"\"\" Create Batches \"\"\"\n",
    "batch_size = 500\n",
    "path_2_batch = path_2 + 'batches/'\n",
    "\n",
    "uncoded = pd.read_pickle(path_2 + 'uncoded.pkl').reset_index()\n",
    "uncoded['parsed'] = uncoded.index\n",
    "\n",
    "id_list = uncoded.index\n",
    "batch_ids = [id_list[x:x + batch_size] for x in range(0, len(id_list), batch_size)]\n",
    "#batches = zip(range(len(batch_ids)), batch_ids, [print_log for b in batch_ids])\n",
    "batches = list(zip(range(len(batch_ids)), batch_ids))[:10]\n",
    "finished_batches = os.listdir(path_2_batch)\n",
    "\n",
    "\"\"\" Step 4.a | Geocode Batches \"\"\"\n",
    "\n",
    "t0 = time.time()\n",
    "#print_log[file]['log'].append(f'  Step 3 | Geocode Batches ({len(run_batches)} of {len(batches)})')\n",
    "#printer(print_log)\n",
    "\n",
    "def batch_geocode(batch):\n",
    "    \"\"\" Take a list of idus and geocode them to a file. \"\"\"\n",
    "\n",
    "    #batch_num, batch_idus, printer, print_log = batch\n",
    "    batch_num, batch_idus = batch\n",
    "    batch_path = f'{path_2_batch}{batch_num}.csv'\n",
    "    send_geo = uncoded[uncoded.index.isin(batch_idus)]\n",
    "    #send_geo['id'] = send_geo.index\n",
    "    send_geo.to_csv(batch_path, index=False, header=False)\n",
    "    try:\n",
    "        results = cg.addressbatch(batch_path)\n",
    "        results = pd.DataFrame(results)\n",
    "        results.lat = results.lat.astype(float)\n",
    "        results.lon = results.lon.astype(float)\n",
    "        \n",
    "        results_path = batch_path.replace('.csv','.pkl')\n",
    "        results.to_pickle(results_path)\n",
    "        #os.remove(batch_path)\n",
    "    except:\n",
    "        #os.remove(batch_path)\n",
    "        print(batch_num)\n",
    "        #error_batches.append(batch)\n",
    "    finished_batches = [batch for batch in os.listdir(path_2_batch) if 'pkl' in batch]\n",
    "    #if len(finished_batches)%10 == 0:\n",
    "        #print_log[file]['sublog'] = [f'   || Finished {len(finished_batches)}']\n",
    "        #printer(print_log)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with mp.Pool(processes = 500) as pool:\n",
    "        pool.map(batch_geocode, batches)\n",
    "\n",
    "#print_log[file]['sublog'] = ''\n",
    "#runtime = round(( time.time() - t0 ) / 60 )\n",
    "#print_log[file]['log'][-1] = f'  Step 3 | Geocode Batches ({len(run_batches)} of {len(batches)}) in {runtime} mins'\n",
    "#print_log[file]['log'].append(f'    Batch Errors | {len(error_batches)}')\n",
    "#printer(print_log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Merge \"\"\"\n",
    "\n",
    "from data2_geocode import *\n",
    "\n",
    "merge_files = [x for x in os.listdir(path_2 + 'batches/') if '.pkl' in x]\n",
    "\n",
    "f = []\n",
    "for file in merge_files:\n",
    "    try:\n",
    "        f.append(pd.read_pickle(path_2 + 'batches/' + file))\n",
    "    except:\n",
    "        pass\n",
    "merged = pd.concat(f)\n",
    "merged = merged[merged.matchtype.isin(['Exact', 'Non_Exact'])]\n",
    "\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "savedate = ''.join([str(today.year),str(today.strftime('%m')),str(today.strftime('%d'))])\n",
    "\n",
    "merged.to_pickle(path_2 + 'batches/merged_' + savedate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data2_geocode import *\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded = pd.read_pickle(path_2+'geocoded.pkl')\n",
    "geocoded = geocoded.drop_duplicates(subset=['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = pd.read_pickle(path_2+'addresses.pkl')\n",
    "addresses = addresses.drop_duplicates(subset=['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "savedate = ''.join([str(today.year),str(today.strftime('%m')),str(today.strftime('%d'))])\n",
    "merged = pd.read_pickle(path_2 + 'batches/merged_' + savedate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_cols = ['matchtype', 'parsed', 'tigerlineid', 'side', 'statefp', 'countyfp', 'tract', 'block', 'lat', 'lon']\n",
    "merged = merged[geo_cols].drop_duplicates(subset=['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses.index = addresses.parsed\n",
    "merged.index = merged.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoder = pd.concat([addresses, merged], axis=1, join=\"inner\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoder.to_pickle(path_2 + 'batches/recoded_' + savedate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoder = pd.read_pickle(path_2 + 'batches/recoded_' + savedate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoder = recoder.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoder = recoder.drop_duplicates(subset='parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder = pd.read_pickle(path_2 + 'coder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder = coder.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coder = coder.drop_duplicates(subset=['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder.index = coder.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recoder = recoder[~recoder.index.isin(coder.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoder.index = recoder.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2306053/606449139.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_coder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0mobj_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                         \u001b[0mindexers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3442\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requires_unique_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_interval_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "new_coder = pd.concat([coder, recoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tried and True Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20181106\n",
      "  Step 1 | Open Chunked Data (Runtime: 0 mins)\n",
      "  Step 2 | Create Batches (Runtime: 0 mins)\n",
      "  Step 3 | Geocode Batches (31818 of 31818) in 156 mins\n",
      "    Batch Errors | 21\n",
      "  Step 4 | Merge Batches (Runtime: 1 mins)\n",
      "2019\n",
      "  Step 1 | Open Chunked Data (Runtime: 7 mins)\n",
      "  Step 2 | Create Batches (Runtime: 0 mins)\n",
      "  Step 3 | Geocode Batches (31071 of 31071) in 741 mins\n",
      "    Batch Errors | 1293\n",
      "  Step 4 | Merge Batches (Runtime: 1 mins)\n",
      "20190430\n",
      "  Step 1 | Open Chunked Data (Runtime: 1 mins)\n",
      "  Step 2 | Create Batches (Runtime: 0 mins)\n",
      "  Step 3 | Geocode Batches (29961 of 29961) in 153 mins\n",
      "    Batch Errors | 59\n",
      "  Step 4 | Merge Batches (Runtime: 1 mins)\n",
      "20190514\n",
      "  Step 1 | Open Chunked Data (Runtime: 1 mins)\n",
      "  Step 2 | Create Batches (Runtime: 0 mins)\n",
      "  Step 3 | Geocode Batches (29961 of 29961) in 170 mins\n",
      "    Batch Errors | 82\n",
      "  Step 4 | Merge Batches (Runtime: 1 mins)\n",
      "20190709\n",
      "  Step 1 | Open Chunked Data (Runtime: 1 mins)\n",
      "  Step 2 | Create Batches (Runtime: 0 mins)\n",
      "  Step 3 | Geocode Batches (25906 of 25906) in 135 mins\n",
      "    Batch Errors | 81\n",
      "  Step 4 | Merge Batches (Runtime: 0 mins)\n",
      "20190910\n",
      "  Step 1 | Open Chunked Data (Runtime: 1 mins)\n",
      "  Step 2 | Create Batches (Runtime: 0 mins)\n",
      "  Step 3 | Geocode Batches (30356 of 30356) in 165 mins\n",
      "    Batch Errors | 111\n",
      "  Step 4 | Merge Batches (Runtime: 0 mins)\n",
      "20191008\n",
      "  Step 1 | Open Chunked Data (Runtime: 1 mins)\n",
      "  Step 2 | Create Batches (Runtime: 0 mins)\n",
      "  Step 3 | Geocode Batches (30475 of 30475) in 181 mins\n",
      "    Batch Errors | 81\n",
      "  Step 4 | Merge Batches (Runtime: 1 mins)\n",
      "20191105\n",
      "  Step 1 | Open Chunked Data (Runtime: 1 mins)\n",
      "  Step 2 | Create Batches (Runtime: 0 mins)\n",
      "  Step 3 | Geocode Batches (30546 of 30546)\n",
      "   || Finished 24930\n"
     ]
    }
   ],
   "source": [
    "from data2_geocode import *\n",
    "\n",
    "\"\"\" Step 0 | Setup \"\"\"\n",
    "batch_size = 250\n",
    "\n",
    "future_files = set([x.split('_chunk_')[0] for x in os.listdir(path_1) if 'pkl' in x])\n",
    "files = [file for file in future_files if f'{file}_geo.pkl' not in os.listdir(path_2)]\n",
    "for file in sorted(files):\n",
    "    year = file\n",
    "    save_file = f'{file}_geo.pkl'\n",
    "    print_log[file] = {'log':[],'sublog':[]}\n",
    "    printer(print_log)\n",
    "    \n",
    "    if save_file in os.listdir(path_2):\n",
    "        print_log[file]['log'].append('  Done')\n",
    "        printer(print_log)\n",
    "    if int(year) < 2010:\n",
    "        print_log[file]['log'].append('  Too Old (for now)')\n",
    "        printer(print_log)\n",
    "    \n",
    "    if (save_file not in os.listdir(path_2)) & (int(year) >= 2010):\n",
    "        path_2_batch = f'{path_2}{year}_batches/'\n",
    "        if not os.path.exists(path_2_batch):\n",
    "            os.makedirs(path_2_batch)\n",
    "        \n",
    "        \"\"\" Step 1 | Open Chunked Data \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 1 | Open Chunked Data')\n",
    "        printer(print_log)\n",
    "        chunk_paths, chunk_files = [f'{path_1}{chunk}' for chunk in os.listdir(path_1) if file in chunk], []\n",
    "        for chunk_path in chunk_paths:\n",
    "            chunk = pd.read_pickle(chunk_path)\n",
    "            #with open(chunk_path,'rb') as f: \n",
    "            #    chunk = pickle.load(f)\n",
    "            chunk = chunk[['idu','address','city','state','zipcode']]\n",
    "            chunk_files.append(chunk)\n",
    "        voters = pd.concat(chunk_files, ignore_index=True)\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 1 | Open Chunked Data (Runtime: {runtime} mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 2 | Create Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 2 | Create Batches')\n",
    "        printer(print_log)\n",
    "        id_list = voters.idu.unique()\n",
    "        batch_ids = [id_list[x:x + batch_size] for x in range(0, len(id_list), batch_size)]\n",
    "        batches = list(zip(range(len(batch_ids)), batch_ids, [print_log for b in batch_ids]))\n",
    "        finished_batches = os.listdir(path_2_batch)\n",
    "        run_batches, error_batches = [(b,d,printer,pl) for (b,d,pl) in batches if f'{b}.pkl' not in finished_batches], []\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 2 | Create Batches (Runtime: {runtime} mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 3 | Geocode Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append(f'  Step 3 | Geocode Batches ({len(run_batches)} of {len(batches)})')\n",
    "        printer(print_log)\n",
    "        \n",
    "        def batch_geocode(batch):\n",
    "            \"\"\" Take a list of idus and geocode them to a file. \"\"\"\n",
    "            \n",
    "            batch_num, batch_idus, printer, print_log = batch\n",
    "            batch_path = f'{path_2_batch}{batch_num}.csv'\n",
    "            send_geo = voters[voters.idu.isin(batch_idus)]\n",
    "            send_geo.to_csv(batch_path, index=False, header=False)\n",
    "            try:\n",
    "                results = cg.addressbatch(batch_path)\n",
    "                results = pd.DataFrame(results)\n",
    "                results.lat = results.lat.astype(float)\n",
    "                results.lon = results.lon.astype(float)\n",
    "                with open(batch_path.replace('.csv','.pkl'),'wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "                os.remove(batch_path)\n",
    "            except:\n",
    "                error_batches.append(batch)\n",
    "            finished_batches = [batch for batch in os.listdir(path_2_batch) if 'pkl' in batch]\n",
    "            if len(finished_batches)%10 == 0:\n",
    "                print_log[file]['sublog'] = [f'   || Finished {len(finished_batches)}']\n",
    "                printer(print_log)\n",
    "        \n",
    "        if __name__ == '__main__':\n",
    "            with mp.Pool(processes = 500) as pool:\n",
    "                pool.map(batch_geocode, run_batches)\n",
    "        \n",
    "        print_log[file]['sublog'] = ''\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 3 | Geocode Batches ({len(run_batches)} of {len(batches)}) in {runtime} mins'\n",
    "        print_log[file]['log'].append(f'    Batch Errors | {len(error_batches)}')\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 4 | Merge Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 4 | Merge Batches')\n",
    "        printer(print_log)\n",
    "        geo_paths, geodata_to_merge = [f'{path_2_batch}{batch}' for batch in os.listdir(path_2_batch) if 'pkl' in batch], []\n",
    "        for geo_path in geo_paths:\n",
    "            if os.path.getsize(geo_path) > 0:\n",
    "                with open(geo_path,'rb') as f: \n",
    "                    results = pickle.load(f)\n",
    "                    geodata_to_merge.append(results)\n",
    "        geodata = pd.concat(geodata_to_merge, ignore_index=True)\n",
    "        geodata = geodata.rename(columns = {'id':'idu'})\n",
    "        with open(f'{path_2}{file}_geo.pkl','wb') as f:\n",
    "            pickle.dump(geodata, f)\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 4 | Merge Batches (Runtime: {runtime} mins)'\n",
    "        printer(print_log)\n",
    "\n",
    "        \"\"\" Saving print_log \"\"\"\n",
    "        now = datetime.now()\n",
    "        savedate = ''.join([str(now.year),str(now.strftime('%m')),str(now.strftime('%d'))])\n",
    "        file = open(f'{path_2}print_log_{savedate}.txt', 'w')\n",
    "        file.write(string_printer(print_log))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Fixing Batch Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n"
     ]
    }
   ],
   "source": [
    "from data2_geocode import *\n",
    "\n",
    "\"\"\" Step 0 | Setup \"\"\"\n",
    "\n",
    "future_files = set([x.split('_chunk_')[0] for x in os.listdir(path_1) if 'pkl' in x])\n",
    "files = [file for file in future_files if f'{file}_geo.pkl']# not in os.listdir(path_2)]\n",
    "file = sorted(files)[0]\n",
    "print(file)\n",
    "year = file\n",
    "\n",
    "path_2_batch = f'{path_2}{year}_batches/'\n",
    "\n",
    "chunk_paths, chunk_files = [f'{path_1}{chunk}' for chunk in os.listdir(path_1) if file in chunk], []\n",
    "for chunk_path in chunk_paths[:1]:\n",
    "    chunk = pd.read_pickle(chunk_path)\n",
    "    #with open(chunk_path,'rb') as f: \n",
    "    #    chunk = pickle.load(f)\n",
    "    #chunk = chunk[['idu','address','city','state','zipcode']]\n",
    "    chunk.index = chunk.idu\n",
    "    chunk_files.append(chunk)\n",
    "#voters = pd.concat(chunk_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_number</th>\n",
       "      <th>street_name</th>\n",
       "      <th>street_type</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idu</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DS1391414721</th>\n",
       "      <td>900</td>\n",
       "      <td>FERN</td>\n",
       "      <td>ST</td>\n",
       "      <td>NC</td>\n",
       "      <td>ROCKINGHAM</td>\n",
       "      <td>MADISON</td>\n",
       "      <td>27025</td>\n",
       "      <td>900 FERN ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DS1388914688</th>\n",
       "      <td>818</td>\n",
       "      <td>RIDGE</td>\n",
       "      <td>ST</td>\n",
       "      <td>NC</td>\n",
       "      <td>ROCKINGHAM</td>\n",
       "      <td>MADISON</td>\n",
       "      <td>27025</td>\n",
       "      <td>818 RIDGE ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DS1389014689</th>\n",
       "      <td>818</td>\n",
       "      <td>RIDGE</td>\n",
       "      <td>ST</td>\n",
       "      <td>NC</td>\n",
       "      <td>ROCKINGHAM</td>\n",
       "      <td>MADISON</td>\n",
       "      <td>27025</td>\n",
       "      <td>818 RIDGE ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DS1393914755</th>\n",
       "      <td>119</td>\n",
       "      <td>VALLEY FIELD</td>\n",
       "      <td>RD</td>\n",
       "      <td>NC</td>\n",
       "      <td>ROCKINGHAM</td>\n",
       "      <td>STONEVILLE</td>\n",
       "      <td>27048</td>\n",
       "      <td>119 VALLEY FIELD RD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DS1393814754</th>\n",
       "      <td>1229</td>\n",
       "      <td>ELLERBE</td>\n",
       "      <td>CT</td>\n",
       "      <td>NC</td>\n",
       "      <td>ROCKINGHAM</td>\n",
       "      <td>EDEN</td>\n",
       "      <td>27288</td>\n",
       "      <td>1229 ELLERBE CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH776021100057436</th>\n",
       "      <td>8320</td>\n",
       "      <td>SHILOH CREEK</td>\n",
       "      <td>CT</td>\n",
       "      <td>NC</td>\n",
       "      <td>WAKE</td>\n",
       "      <td>RALEIGH</td>\n",
       "      <td>27616</td>\n",
       "      <td>8320 SHILOH CREEK CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH776578100058107</th>\n",
       "      <td>801</td>\n",
       "      <td>SOUTHAMPTON</td>\n",
       "      <td>DR</td>\n",
       "      <td>NC</td>\n",
       "      <td>WAKE</td>\n",
       "      <td>KNIGHTDALE</td>\n",
       "      <td>27545</td>\n",
       "      <td>801 SOUTHAMPTON DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH776579100058109</th>\n",
       "      <td>5412</td>\n",
       "      <td>ORCHARD ORIOLE</td>\n",
       "      <td>TRL</td>\n",
       "      <td>NC</td>\n",
       "      <td>WAKE</td>\n",
       "      <td>WAKE FOREST</td>\n",
       "      <td>27587</td>\n",
       "      <td>5412 ORCHARD ORIOLE TRL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH776580100058110</th>\n",
       "      <td>2813</td>\n",
       "      <td>FERRETT</td>\n",
       "      <td>CT</td>\n",
       "      <td>NC</td>\n",
       "      <td>WAKE</td>\n",
       "      <td>RALEIGH</td>\n",
       "      <td>27610</td>\n",
       "      <td>2813 FERRETT CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH768904100058111</th>\n",
       "      <td>315</td>\n",
       "      <td>MEREDITH</td>\n",
       "      <td>ST</td>\n",
       "      <td>NC</td>\n",
       "      <td>WAKE</td>\n",
       "      <td>RALEIGH</td>\n",
       "      <td>27606</td>\n",
       "      <td>315 MEREDITH ST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>698551 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  house_number     street_name street_type state      county  \\\n",
       "idu                                                                            \n",
       "DS1391414721               900            FERN          ST    NC  ROCKINGHAM   \n",
       "DS1388914688               818           RIDGE          ST    NC  ROCKINGHAM   \n",
       "DS1389014689               818           RIDGE          ST    NC  ROCKINGHAM   \n",
       "DS1393914755               119    VALLEY FIELD          RD    NC  ROCKINGHAM   \n",
       "DS1393814754              1229         ELLERBE          CT    NC  ROCKINGHAM   \n",
       "...                        ...             ...         ...   ...         ...   \n",
       "EH776021100057436         8320    SHILOH CREEK          CT    NC        WAKE   \n",
       "EH776578100058107          801     SOUTHAMPTON          DR    NC        WAKE   \n",
       "EH776579100058109         5412  ORCHARD ORIOLE         TRL    NC        WAKE   \n",
       "EH776580100058110         2813         FERRETT          CT    NC        WAKE   \n",
       "EH768904100058111          315        MEREDITH          ST    NC        WAKE   \n",
       "\n",
       "                          city zipcode                  address  \n",
       "idu                                                              \n",
       "DS1391414721           MADISON   27025              900 FERN ST  \n",
       "DS1388914688           MADISON   27025             818 RIDGE ST  \n",
       "DS1389014689           MADISON   27025             818 RIDGE ST  \n",
       "DS1393914755        STONEVILLE   27048      119 VALLEY FIELD RD  \n",
       "DS1393814754              EDEN   27288          1229 ELLERBE CT  \n",
       "...                        ...     ...                      ...  \n",
       "EH776021100057436      RALEIGH   27616     8320 SHILOH CREEK CT  \n",
       "EH776578100058107   KNIGHTDALE   27545       801 SOUTHAMPTON DR  \n",
       "EH776579100058109  WAKE FOREST   27587  5412 ORCHARD ORIOLE TRL  \n",
       "EH776580100058110      RALEIGH   27610          2813 FERRETT CT  \n",
       "EH768904100058111      RALEIGH   27606          315 MEREDITH ST  \n",
       "\n",
       "[698551 rows x 8 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk[['house_number', 'street_name', 'street_type', 'state', 'county',\n",
    "       'city', 'zipcode', 'address']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "id_list = list(set(voters.index))\n",
    "batch_ids = [id_list[x:x + batch_size] for x in range(0, len(id_list), batch_size)]\n",
    "batches = list(zip(range(len(batch_ids)), batch_ids, [print_log for b in batch_ids]))\n",
    "run_batches, error_batches = [(b,d,printer,pl) for (b,d,pl) in batches if f'{b}.pkl'], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>address</th>\n",
       "      <th>match</th>\n",
       "      <th>matchtype</th>\n",
       "      <th>parsed</th>\n",
       "      <th>tigerlineid</th>\n",
       "      <th>side</th>\n",
       "      <th>statefp</th>\n",
       "      <th>countyfp</th>\n",
       "      <th>tract</th>\n",
       "      <th>block</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CZ6373090845</td>\n",
       "      <td>155 LAKE FOREST DR, PINEHURST, NC, 28374</td>\n",
       "      <td>True</td>\n",
       "      <td>Exact</td>\n",
       "      <td>155 LAKE FOREST DR, PINEHURST, NC, 28374</td>\n",
       "      <td>20298847</td>\n",
       "      <td>L</td>\n",
       "      <td>37</td>\n",
       "      <td>125</td>\n",
       "      <td>950704</td>\n",
       "      <td>2015</td>\n",
       "      <td>35.184982</td>\n",
       "      <td>-79.485830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CJ625023034407</td>\n",
       "      <td>605 WOOD ST, SELMA, NC, 27576</td>\n",
       "      <td>True</td>\n",
       "      <td>Exact</td>\n",
       "      <td>605 WOOD ST, SELMA, NC, 27576</td>\n",
       "      <td>26525633</td>\n",
       "      <td>L</td>\n",
       "      <td>37</td>\n",
       "      <td>101</td>\n",
       "      <td>040301</td>\n",
       "      <td>1005</td>\n",
       "      <td>35.540028</td>\n",
       "      <td>-78.273460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BR1259217615303</td>\n",
       "      <td>1204 SOUTH POINT RD, BELMONT, NC, 28012</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AB2514629796</td>\n",
       "      <td>180 CEDAR RD, HICKORY, NC, 28601</td>\n",
       "      <td>True</td>\n",
       "      <td>Exact</td>\n",
       "      <td>180 CEDAR RD, HICKORY, NC, 28601</td>\n",
       "      <td>45793069</td>\n",
       "      <td>R</td>\n",
       "      <td>37</td>\n",
       "      <td>003</td>\n",
       "      <td>040700</td>\n",
       "      <td>3006</td>\n",
       "      <td>35.812172</td>\n",
       "      <td>-81.310840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BF2252724498</td>\n",
       "      <td>103 RED WOOD ST, MOYOCK, NC, 27958</td>\n",
       "      <td>True</td>\n",
       "      <td>Exact</td>\n",
       "      <td>103 RED WOOD ST, MOYOCK, NC, 27958</td>\n",
       "      <td>81385223</td>\n",
       "      <td>R</td>\n",
       "      <td>37</td>\n",
       "      <td>053</td>\n",
       "      <td>110301</td>\n",
       "      <td>1000</td>\n",
       "      <td>36.478210</td>\n",
       "      <td>-76.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>BW931110009</td>\n",
       "      <td>201 ALLEN ST, CREEDMOOR, NC, 27522</td>\n",
       "      <td>True</td>\n",
       "      <td>Exact</td>\n",
       "      <td>201 ALLEN ST, CREEDMOOR, NC, 27522</td>\n",
       "      <td>77106736</td>\n",
       "      <td>L</td>\n",
       "      <td>37</td>\n",
       "      <td>077</td>\n",
       "      <td>970606</td>\n",
       "      <td>2006</td>\n",
       "      <td>36.117520</td>\n",
       "      <td>-78.681526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>BN29499630028675</td>\n",
       "      <td>3401 OLD VINEYARD RD, WINSTON SALEM, NC, 27103</td>\n",
       "      <td>True</td>\n",
       "      <td>Non_Exact</td>\n",
       "      <td>3401 OLD VINEYARD RD, WINSTON SALEM, NC, 27103</td>\n",
       "      <td>642174510</td>\n",
       "      <td>R</td>\n",
       "      <td>37</td>\n",
       "      <td>067</td>\n",
       "      <td>003805</td>\n",
       "      <td>1004</td>\n",
       "      <td>36.077100</td>\n",
       "      <td>-80.311190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>AS6633475725</td>\n",
       "      <td>311 FAIRVIEW DR, EMERALD ISLE, NC, 28594</td>\n",
       "      <td>True</td>\n",
       "      <td>Exact</td>\n",
       "      <td>311 FAIRVIEW DR, EMERALD ISLE, NC, 28594</td>\n",
       "      <td>33865571</td>\n",
       "      <td>L</td>\n",
       "      <td>37</td>\n",
       "      <td>031</td>\n",
       "      <td>970904</td>\n",
       "      <td>1017</td>\n",
       "      <td>34.667300</td>\n",
       "      <td>-77.036460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>AW1180059093384</td>\n",
       "      <td>1207 21ST AVE, HICKORY, NC, 28601</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>BY362268504829</td>\n",
       "      <td>706 NORTHRIDGE ST, GREENSBORO, NC, 27403</td>\n",
       "      <td>True</td>\n",
       "      <td>Exact</td>\n",
       "      <td>706 NORTHRIDGE ST, GREENSBORO, NC, 27403</td>\n",
       "      <td>131029884</td>\n",
       "      <td>R</td>\n",
       "      <td>37</td>\n",
       "      <td>081</td>\n",
       "      <td>010601</td>\n",
       "      <td>2002</td>\n",
       "      <td>36.066840</td>\n",
       "      <td>-79.831955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                         address  match  \\\n",
       "0       CZ6373090845        155 LAKE FOREST DR, PINEHURST, NC, 28374   True   \n",
       "1     CJ625023034407                   605 WOOD ST, SELMA, NC, 27576   True   \n",
       "2    BR1259217615303         1204 SOUTH POINT RD, BELMONT, NC, 28012  False   \n",
       "3       AB2514629796                180 CEDAR RD, HICKORY, NC, 28601   True   \n",
       "4       BF2252724498              103 RED WOOD ST, MOYOCK, NC, 27958   True   \n",
       "..               ...                                             ...    ...   \n",
       "95       BW931110009              201 ALLEN ST, CREEDMOOR, NC, 27522   True   \n",
       "96  BN29499630028675  3401 OLD VINEYARD RD, WINSTON SALEM, NC, 27103   True   \n",
       "97      AS6633475725        311 FAIRVIEW DR, EMERALD ISLE, NC, 28594   True   \n",
       "98   AW1180059093384               1207 21ST AVE, HICKORY, NC, 28601  False   \n",
       "99    BY362268504829        706 NORTHRIDGE ST, GREENSBORO, NC, 27403   True   \n",
       "\n",
       "    matchtype                                          parsed tigerlineid  \\\n",
       "0       Exact        155 LAKE FOREST DR, PINEHURST, NC, 28374    20298847   \n",
       "1       Exact                   605 WOOD ST, SELMA, NC, 27576    26525633   \n",
       "2        None                                            None        None   \n",
       "3       Exact                180 CEDAR RD, HICKORY, NC, 28601    45793069   \n",
       "4       Exact              103 RED WOOD ST, MOYOCK, NC, 27958    81385223   \n",
       "..        ...                                             ...         ...   \n",
       "95      Exact              201 ALLEN ST, CREEDMOOR, NC, 27522    77106736   \n",
       "96  Non_Exact  3401 OLD VINEYARD RD, WINSTON SALEM, NC, 27103   642174510   \n",
       "97      Exact        311 FAIRVIEW DR, EMERALD ISLE, NC, 28594    33865571   \n",
       "98       None                                            None        None   \n",
       "99      Exact        706 NORTHRIDGE ST, GREENSBORO, NC, 27403   131029884   \n",
       "\n",
       "    side statefp countyfp   tract block        lat        lon  \n",
       "0      L      37      125  950704  2015  35.184982 -79.485830  \n",
       "1      L      37      101  040301  1005  35.540028 -78.273460  \n",
       "2   None    None     None    None  None        NaN        NaN  \n",
       "3      R      37      003  040700  3006  35.812172 -81.310840  \n",
       "4      R      37      053  110301  1000  36.478210 -76.047000  \n",
       "..   ...     ...      ...     ...   ...        ...        ...  \n",
       "95     L      37      077  970606  2006  36.117520 -78.681526  \n",
       "96     R      37      067  003805  1004  36.077100 -80.311190  \n",
       "97     L      37      031  970904  1017  34.667300 -77.036460  \n",
       "98  None    None     None    None  None        NaN        NaN  \n",
       "99     R      37      081  010601  2002  36.066840 -79.831955  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = batches[10]\n",
    "batch_num, batch_idus, print_log = batch\n",
    "batch_path = f'{path_2_batch}{batch_num}.csv'\n",
    "send_geo = voters[voters.index.isin(batch_idus)]\n",
    "send_geo.to_csv(batch_path, index=False, header=False)\n",
    "results = cg.addressbatch(batch_path)\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BR1259217615303',\n",
       " 'DZ5109030043544',\n",
       " 'DH3267088189',\n",
       " 'AX2590744610',\n",
       " 'BB3846763856',\n",
       " 'BL173802239628',\n",
       " 'BL30683730069560',\n",
       " 'EP1504618909',\n",
       " 'BB865581024767',\n",
       " 'BB1161518574',\n",
       " 'AK4053648623',\n",
       " 'AM2134519322',\n",
       " 'DL3266330694',\n",
       " 'CM3296242233',\n",
       " 'DE2152819506',\n",
       " 'AW555629028918',\n",
       " 'AW1180059093384']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_ids = list(results[results['match'] == False].id)\n",
    "failed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'address', 'match', 'matchtype', 'parsed', 'tigerlineid',\n",
       "       'side', 'statefp', 'countyfp', 'tract', 'block', 'lat', 'lon'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>address</th>\n",
       "      <th>match</th>\n",
       "      <th>matchtype</th>\n",
       "      <th>parsed</th>\n",
       "      <th>tigerlineid</th>\n",
       "      <th>side</th>\n",
       "      <th>statefp</th>\n",
       "      <th>countyfp</th>\n",
       "      <th>tract</th>\n",
       "      <th>block</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BL173802239628</td>\n",
       "      <td>403 LAKE HOGAN FARM RD, CHAPEL HILL, NC, 27516</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BB1161518574</td>\n",
       "      <td>2615 BALL PARK RD, LAWNDALE, NC, 28090</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CM3296242233</td>\n",
       "      <td>303 RURAL RTE 7 , KINSTON, NC, 28501</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DH3267088189</td>\n",
       "      <td>300 MAIN ST, BURGAW, NC, 28425</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BR1259217615303</td>\n",
       "      <td>1204 SOUTH POINT RD, BELMONT, NC, 28012</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DE2152819506</td>\n",
       "      <td>1205 THE OAKS APTS , CHAPEL HILL, NC, 27514</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DZ5109030043544</td>\n",
       "      <td>185 CORBAN AVE, CONCORD, NC, 28025</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BL30683730069560</td>\n",
       "      <td>1 DUKE UNIVERSITY WEST CAMPUS DORM, DURHAM, NC...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AK4053648623</td>\n",
       "      <td>10161 CREEKSIDE DR, LELAND, NC, 28451</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AW555629028918</td>\n",
       "      <td>0 ROUTE 1 , CONOVER, NC, 28613</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AX2590744610</td>\n",
       "      <td>190 C D THOMAS RD, SILER CITY, NC, 27344</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AM2134519322</td>\n",
       "      <td>0 DYSARTSVILLE RD, MORGANTON, NC, 28655</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DL3266330694</td>\n",
       "      <td>206 GORDON ST, GRIFTON, NC, 28530</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BB3846763856</td>\n",
       "      <td>1956 PLEASANT HILL CH RD, SHELBY, NC, 28152</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EP1504618909</td>\n",
       "      <td>900 WAINWRIGHT AVE, WILSON, NC, 27893</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AW1180059093384</td>\n",
       "      <td>1207 21ST AVE, HICKORY, NC, 28601</td>\n",
       "      <td>True</td>\n",
       "      <td>Non_Exact</td>\n",
       "      <td>1207 21ST AVE NE, HICKORY, NC, 28601</td>\n",
       "      <td>45954777</td>\n",
       "      <td>L</td>\n",
       "      <td>37</td>\n",
       "      <td>035</td>\n",
       "      <td>010403</td>\n",
       "      <td>1023</td>\n",
       "      <td>35.757534</td>\n",
       "      <td>-81.31491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BB865581024767</td>\n",
       "      <td>3814 HARRIS CREEK RD, LAWNDALE, NC, 28090</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                            address  \\\n",
       "0     BL173802239628     403 LAKE HOGAN FARM RD, CHAPEL HILL, NC, 27516   \n",
       "1       BB1161518574             2615 BALL PARK RD, LAWNDALE, NC, 28090   \n",
       "2       CM3296242233               303 RURAL RTE 7 , KINSTON, NC, 28501   \n",
       "3       DH3267088189                     300 MAIN ST, BURGAW, NC, 28425   \n",
       "4    BR1259217615303            1204 SOUTH POINT RD, BELMONT, NC, 28012   \n",
       "5       DE2152819506        1205 THE OAKS APTS , CHAPEL HILL, NC, 27514   \n",
       "6    DZ5109030043544                 185 CORBAN AVE, CONCORD, NC, 28025   \n",
       "7   BL30683730069560  1 DUKE UNIVERSITY WEST CAMPUS DORM, DURHAM, NC...   \n",
       "8       AK4053648623              10161 CREEKSIDE DR, LELAND, NC, 28451   \n",
       "9     AW555629028918                     0 ROUTE 1 , CONOVER, NC, 28613   \n",
       "10      AX2590744610           190 C D THOMAS RD, SILER CITY, NC, 27344   \n",
       "11      AM2134519322            0 DYSARTSVILLE RD, MORGANTON, NC, 28655   \n",
       "12      DL3266330694                  206 GORDON ST, GRIFTON, NC, 28530   \n",
       "13      BB3846763856        1956 PLEASANT HILL CH RD, SHELBY, NC, 28152   \n",
       "14      EP1504618909              900 WAINWRIGHT AVE, WILSON, NC, 27893   \n",
       "15   AW1180059093384                  1207 21ST AVE, HICKORY, NC, 28601   \n",
       "16    BB865581024767          3814 HARRIS CREEK RD, LAWNDALE, NC, 28090   \n",
       "\n",
       "    match  matchtype                                parsed tigerlineid  side  \\\n",
       "0   False       None                                  None        None  None   \n",
       "1   False       None                                  None        None  None   \n",
       "2   False       None                                  None        None  None   \n",
       "3   False       None                                  None        None  None   \n",
       "4   False       None                                  None        None  None   \n",
       "5   False       None                                  None        None  None   \n",
       "6   False       None                                  None        None  None   \n",
       "7   False       None                                  None        None  None   \n",
       "8   False       None                                  None        None  None   \n",
       "9   False       None                                  None        None  None   \n",
       "10  False       None                                  None        None  None   \n",
       "11  False       None                                  None        None  None   \n",
       "12  False       None                                  None        None  None   \n",
       "13  False       None                                  None        None  None   \n",
       "14  False       None                                  None        None  None   \n",
       "15   True  Non_Exact  1207 21ST AVE NE, HICKORY, NC, 28601    45954777     L   \n",
       "16  False       None                                  None        None  None   \n",
       "\n",
       "   statefp countyfp   tract block        lat       lon  \n",
       "0     None     None    None  None        NaN       NaN  \n",
       "1     None     None    None  None        NaN       NaN  \n",
       "2     None     None    None  None        NaN       NaN  \n",
       "3     None     None    None  None        NaN       NaN  \n",
       "4     None     None    None  None        NaN       NaN  \n",
       "5     None     None    None  None        NaN       NaN  \n",
       "6     None     None    None  None        NaN       NaN  \n",
       "7     None     None    None  None        NaN       NaN  \n",
       "8     None     None    None  None        NaN       NaN  \n",
       "9     None     None    None  None        NaN       NaN  \n",
       "10    None     None    None  None        NaN       NaN  \n",
       "11    None     None    None  None        NaN       NaN  \n",
       "12    None     None    None  None        NaN       NaN  \n",
       "13    None     None    None  None        NaN       NaN  \n",
       "14    None     None    None  None        NaN       NaN  \n",
       "15      37      035  010403  1023  35.757534 -81.31491  \n",
       "16    None     None    None  None        NaN       NaN  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_geo = voters[voters.index.isin(failed_ids)]\n",
    "send_geo.to_csv(batch_path.replace('.csv','_.csv'), index=False, header=False)\n",
    "results = cg.addressbatch(batch_path.replace('.csv','_.csv'))\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idu</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idu</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BR1259217615303</th>\n",
       "      <td>BR1259217615303</td>\n",
       "      <td>1204 SOUTH POINT RD</td>\n",
       "      <td>BELMONT</td>\n",
       "      <td>NC</td>\n",
       "      <td>28012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CM3296242233</th>\n",
       "      <td>CM3296242233</td>\n",
       "      <td>303 RURAL RTE 7</td>\n",
       "      <td>KINSTON</td>\n",
       "      <td>NC</td>\n",
       "      <td>28501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BL30683730069560</th>\n",
       "      <td>BL30683730069560</td>\n",
       "      <td>1 DUKE UNIVERSITY WEST CAMPUS DORM</td>\n",
       "      <td>DURHAM</td>\n",
       "      <td>NC</td>\n",
       "      <td>27708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE2152819506</th>\n",
       "      <td>DE2152819506</td>\n",
       "      <td>1205 THE OAKS APTS</td>\n",
       "      <td>CHAPEL HILL</td>\n",
       "      <td>NC</td>\n",
       "      <td>27514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AW555629028918</th>\n",
       "      <td>AW555629028918</td>\n",
       "      <td>0 ROUTE 1</td>\n",
       "      <td>CONOVER</td>\n",
       "      <td>NC</td>\n",
       "      <td>28613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AW1180059093384</th>\n",
       "      <td>AW1180059093384</td>\n",
       "      <td>1207 21ST AVE</td>\n",
       "      <td>HICKORY</td>\n",
       "      <td>NC</td>\n",
       "      <td>28601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AX2590744610</th>\n",
       "      <td>AX2590744610</td>\n",
       "      <td>190 C D THOMAS RD</td>\n",
       "      <td>SILER CITY</td>\n",
       "      <td>NC</td>\n",
       "      <td>27344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BB865581024767</th>\n",
       "      <td>BB865581024767</td>\n",
       "      <td>3814 HARRIS CREEK RD</td>\n",
       "      <td>LAWNDALE</td>\n",
       "      <td>NC</td>\n",
       "      <td>28090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BB1161518574</th>\n",
       "      <td>BB1161518574</td>\n",
       "      <td>2615 BALL PARK RD</td>\n",
       "      <td>LAWNDALE</td>\n",
       "      <td>NC</td>\n",
       "      <td>28090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BB3846763856</th>\n",
       "      <td>BB3846763856</td>\n",
       "      <td>1956 PLEASANT HILL CH RD</td>\n",
       "      <td>SHELBY</td>\n",
       "      <td>NC</td>\n",
       "      <td>28152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK4053648623</th>\n",
       "      <td>AK4053648623</td>\n",
       "      <td>10161 CREEKSIDE DR</td>\n",
       "      <td>LELAND</td>\n",
       "      <td>NC</td>\n",
       "      <td>28451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AM2134519322</th>\n",
       "      <td>AM2134519322</td>\n",
       "      <td>0 DYSARTSVILLE RD</td>\n",
       "      <td>MORGANTON</td>\n",
       "      <td>NC</td>\n",
       "      <td>28655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DZ5109030043544</th>\n",
       "      <td>DZ5109030043544</td>\n",
       "      <td>185 CORBAN AVE</td>\n",
       "      <td>CONCORD</td>\n",
       "      <td>NC</td>\n",
       "      <td>28025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EP1504618909</th>\n",
       "      <td>EP1504618909</td>\n",
       "      <td>900 WAINWRIGHT AVE</td>\n",
       "      <td>WILSON</td>\n",
       "      <td>NC</td>\n",
       "      <td>27893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BL173802239628</th>\n",
       "      <td>BL173802239628</td>\n",
       "      <td>403 LAKE HOGAN FARM RD</td>\n",
       "      <td>CHAPEL HILL</td>\n",
       "      <td>NC</td>\n",
       "      <td>27516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DH3267088189</th>\n",
       "      <td>DH3267088189</td>\n",
       "      <td>300 MAIN ST</td>\n",
       "      <td>BURGAW</td>\n",
       "      <td>NC</td>\n",
       "      <td>28425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL3266330694</th>\n",
       "      <td>DL3266330694</td>\n",
       "      <td>206 GORDON ST</td>\n",
       "      <td>GRIFTON</td>\n",
       "      <td>NC</td>\n",
       "      <td>28530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               idu                             address  \\\n",
       "idu                                                                      \n",
       "BR1259217615303    BR1259217615303                 1204 SOUTH POINT RD   \n",
       "CM3296242233          CM3296242233                    303 RURAL RTE 7    \n",
       "BL30683730069560  BL30683730069560  1 DUKE UNIVERSITY WEST CAMPUS DORM   \n",
       "DE2152819506          DE2152819506                 1205 THE OAKS APTS    \n",
       "AW555629028918      AW555629028918                          0 ROUTE 1    \n",
       "AW1180059093384    AW1180059093384                       1207 21ST AVE   \n",
       "AX2590744610          AX2590744610                   190 C D THOMAS RD   \n",
       "BB865581024767      BB865581024767                3814 HARRIS CREEK RD   \n",
       "BB1161518574          BB1161518574                   2615 BALL PARK RD   \n",
       "BB3846763856          BB3846763856            1956 PLEASANT HILL CH RD   \n",
       "AK4053648623          AK4053648623                  10161 CREEKSIDE DR   \n",
       "AM2134519322          AM2134519322                   0 DYSARTSVILLE RD   \n",
       "DZ5109030043544    DZ5109030043544                      185 CORBAN AVE   \n",
       "EP1504618909          EP1504618909                  900 WAINWRIGHT AVE   \n",
       "BL173802239628      BL173802239628              403 LAKE HOGAN FARM RD   \n",
       "DH3267088189          DH3267088189                         300 MAIN ST   \n",
       "DL3266330694          DL3266330694                       206 GORDON ST   \n",
       "\n",
       "                         city state zipcode  \n",
       "idu                                          \n",
       "BR1259217615303       BELMONT    NC   28012  \n",
       "CM3296242233          KINSTON    NC   28501  \n",
       "BL30683730069560       DURHAM    NC   27708  \n",
       "DE2152819506      CHAPEL HILL    NC   27514  \n",
       "AW555629028918        CONOVER    NC   28613  \n",
       "AW1180059093384       HICKORY    NC   28601  \n",
       "AX2590744610       SILER CITY    NC   27344  \n",
       "BB865581024767       LAWNDALE    NC   28090  \n",
       "BB1161518574         LAWNDALE    NC   28090  \n",
       "BB3846763856           SHELBY    NC   28152  \n",
       "AK4053648623           LELAND    NC   28451  \n",
       "AM2134519322        MORGANTON    NC   28655  \n",
       "DZ5109030043544       CONCORD    NC   28025  \n",
       "EP1504618909           WILSON    NC   27893  \n",
       "BL173802239628    CHAPEL HILL    NC   27516  \n",
       "DH3267088189           BURGAW    NC   28425  \n",
       "DL3266330694          GRIFTON    NC   28530  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_idu_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relabel(x):\n",
    "    return x.replace('CH','CHURCH')\n",
    "\n",
    "v = send_geo.iloc[10]\n",
    "d = cg.address(relabel(v.address), city=v.city, state=v.state, zipcode=v.zipcode)\n",
    "if len(d)>0: \n",
    "    dd = {\n",
    "        'idu':[v.idu],\n",
    "        'address':[relabel(v.address)],\n",
    "        'city':[v.city],\n",
    "        'state':[v.state],\n",
    "        'zipcode':[v.zipcode],\n",
    "        'parsed':[d[0]['matchedAddress']],\n",
    "        'lat':[d[0]['coordinates']['y']],\n",
    "        'lon':[d[0]['coordinates']['x']]\n",
    "    }\n",
    "    dd_list.append(pd.DataFrame(dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idu</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>parsed</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK4053648623</td>\n",
       "      <td>10161 CREEKSIDE DR</td>\n",
       "      <td>LELAND</td>\n",
       "      <td>NC</td>\n",
       "      <td>28451</td>\n",
       "      <td>10161 CREEKSIDE DR SE, LELAND, NC, 28451</td>\n",
       "      <td>34.198963</td>\n",
       "      <td>-77.98203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            idu             address    city state zipcode  \\\n",
       "0  AK4053648623  10161 CREEKSIDE DR  LELAND    NC   28451   \n",
       "\n",
       "                                     parsed        lat       lon  \n",
       "0  10161 CREEKSIDE DR SE, LELAND, NC, 28451  34.198963 -77.98203  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['id', 'address', 'match', 'matchtype', 'parsed', 'tigerlineid',\n",
    "       'side', 'statefp', 'countyfp', 'tract', 'block', 'lat', 'lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idu</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>parsed</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DH3267088189</td>\n",
       "      <td>300 MAIN ST</td>\n",
       "      <td>BURGAW</td>\n",
       "      <td>NC</td>\n",
       "      <td>28425</td>\n",
       "      <td>300 W MAIN ST, BURGAW, NC, 28425</td>\n",
       "      <td>34.514664</td>\n",
       "      <td>-77.919876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK4053648623</td>\n",
       "      <td>10161 CREEKSIDE DR</td>\n",
       "      <td>LELAND</td>\n",
       "      <td>NC</td>\n",
       "      <td>28451</td>\n",
       "      <td>10161 CREEKSIDE DR SE, LELAND, NC, 28451</td>\n",
       "      <td>34.198963</td>\n",
       "      <td>-77.982030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            idu             address    city state zipcode  \\\n",
       "0  DH3267088189         300 MAIN ST  BURGAW    NC   28425   \n",
       "0  AK4053648623  10161 CREEKSIDE DR  LELAND    NC   28451   \n",
       "\n",
       "                                     parsed        lat        lon  \n",
       "0          300 W MAIN ST, BURGAW, NC, 28425  34.514664 -77.919876  \n",
       "0  10161 CREEKSIDE DR SE, LELAND, NC, 28451  34.198963 -77.982030  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_con = pd.concat(dd_list)\n",
    "dd_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_rename_dict = {\n",
    "    'CH':'CHURCH',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg.onelineaddress('403 LAKE HOGAN FARM RD, CHAPEL HILL, NC, 27516', returntype='locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "voters = pd.read_pickle(path_2 + '2013_geo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20051125\n",
      "  Step 1 | Open Chunked Data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_741248/1856893589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mchunk_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'{path_1}{chunk}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;31m#with open(chunk_path,'rb') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#    chunk = pickle.load(f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;31m# RawIOBase, BufferedIOBase, TextIOBase, TextIOWrapper, mmap]\";\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0;31m# expected \"IO[bytes]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m_frombuffer\u001b[0;34m(buf, dtype, shape, order)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1854\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_frombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1855\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data2_geocode import *\n",
    "\n",
    "\"\"\" Step 0 | Setup \"\"\"\n",
    "batch_size = 500\n",
    "\n",
    "future_files = set([x.split('_chunk_')[0] for x in os.listdir(path_1) if 'pkl' in x])\n",
    "files = [file for file in future_files if f'{file}_geo.pkl' not in os.listdir(path_2)]\n",
    "for file in sorted(files):\n",
    "    year = file\n",
    "    save_file = f'{file}_geo.pkl'\n",
    "    print_log[file] = {'log':[],'sublog':[]}\n",
    "    printer(print_log)\n",
    "    \n",
    "    if save_file in os.listdir(path_2):\n",
    "        print_log[file]['log'].append('  Done')\n",
    "        printer(print_log)\n",
    "    if int(year) < 2010:\n",
    "        print_log[file]['log'].append('  Too Old (for now)')\n",
    "        printer(print_log)\n",
    "    \n",
    "    if (save_file not in os.listdir(path_2)) & (int(year) >= 2010):\n",
    "        path_2_batch = f'{path_2}{year}_batches/'\n",
    "        if not os.path.exists(path_2_batch):\n",
    "            os.makedirs(path_2_batch)\n",
    "        \n",
    "        \"\"\" Step 1 | Open Chunked Data \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 1 | Open Chunked Data')\n",
    "        printer(print_log)\n",
    "        chunk_paths, chunk_files = [f'{path_1}{chunk}' for chunk in os.listdir(path_1) if file in chunk], []\n",
    "        for chunk_path in chunk_paths:\n",
    "            chunk = pd.read_pickle(chunk_path)\n",
    "            #with open(chunk_path,'rb') as f: \n",
    "            #    chunk = pickle.load(f)\n",
    "            chunk = chunk[['idu','address','city','state','zipcode']]\n",
    "            chunk_files.append(chunk)\n",
    "        voters = pd.concat(chunk_files, ignore_index=True)\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 1 | Open Chunked Data (Runtime: {runtime} mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 2 | Create Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 2 | Create Batches')\n",
    "        printer(print_log)\n",
    "        id_list = voters.idu.unique()\n",
    "        batch_ids = [id_list[x:x + batch_size] for x in range(0, len(id_list), batch_size)]\n",
    "        batches = list(zip(range(len(batch_ids)), batch_ids, [print_log for b in batch_ids]))\n",
    "        finished_batches = os.listdir(path_2_batch)\n",
    "        run_batches, error_batches = [(b,d,printer,pl) for (b,d,pl) in batches if f'{b}.pkl' not in finished_batches], []\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 2 | Create Batches (Runtime: {runtime} mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 3 | Geocode Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append(f'  Step 3 | Geocode Batches ({len(run_batches)} of {len(batches)})')\n",
    "        printer(print_log)\n",
    "        \n",
    "        def batch_geocode(batch):\n",
    "            \"\"\" Take a list of idus and geocode them to a file. \"\"\"\n",
    "            \n",
    "            batch_num, batch_idus, printer, print_log = batch\n",
    "            batch_path = f'{path_2_batch}{batch_num}.csv'\n",
    "            send_geo = voters[voters.idu.isin(batch_idus)]\n",
    "            send_geo.to_csv(batch_path, index=False, header=False)\n",
    "            try:\n",
    "                results = cg.addressbatch(batch_path)\n",
    "                results = pd.DataFrame(results)\n",
    "                results.lat = results.lat.astype(float)\n",
    "                results.lon = results.lon.astype(float)\n",
    "                with open(batch_path.replace('.csv','.pkl'),'wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "                os.remove(batch_path)\n",
    "            except:\n",
    "                error_batches.append(batch)\n",
    "            finished_batches = [batch for batch in os.listdir(path_2_batch) if 'pkl' in batch]\n",
    "            if len(finished_batches)%10 == 0:\n",
    "                print_log[file]['sublog'] = [f'   || Finished {len(finished_batches)}']\n",
    "                printer(print_log)\n",
    "        \n",
    "        if __name__ == '__main__':\n",
    "            with mp.Pool(processes = 500) as pool:\n",
    "                pool.map(batch_geocode, run_batches)\n",
    "        \n",
    "        print_log[file]['sublog'] = ''\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 3 | Geocode Batches ({len(run_batches)} of {len(batches)}) in {runtime} mins'\n",
    "        print_log[file]['log'].append(f'    Batch Errors | {len(error_batches)}')\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 4 | Merge Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 4 | Merge Batches')\n",
    "        printer(print_log)\n",
    "        geo_paths, geodata_to_merge = [f'{path_2_batch}{batch}' for batch in os.listdir(path_2_batch) if 'pkl' in batch], []\n",
    "        for geo_path in geo_paths:\n",
    "            if os.path.getsize(geo_path) > 0:\n",
    "                with open(geo_path,'rb') as f: \n",
    "                    results = pickle.load(f)\n",
    "                    geodata_to_merge.append(results)\n",
    "        geodata = pd.concat(geodata_to_merge, ignore_index=True)\n",
    "        geodata = geodata.rename(columns = {'id':'idu'})\n",
    "        with open(f'{path_2}{file}_geo.pkl','wb') as f:\n",
    "            pickle.dump(geodata, f)\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 4 | Merge Batches (Runtime: {runtime} mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 5 | Fix Errors \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 5 | Fix Errors')\n",
    "        printer(print_log)\n",
    "        \n",
    "        geo_id_list = geodata.idu.unique()\n",
    "        fix_list = [idu for idu in id_list if idu not in geo_id_list]\n",
    "        fix_id_batches = [fix_list[x:x + batch_size] for x in range(0, len(fix_list), batch_size)]\n",
    "        batches = list(zip(range(len(fix_id_batches)), fix_id_batches, [print_log for b in fix_id_batches]))\n",
    "        \n",
    "        def fix_errors(batch):\n",
    "            batch_num, batch_idus, print_log = batch\n",
    "            batch_path = f'{path_2_batch}{batch_num}_fix'\n",
    "            fix_voters = voters[voters.idu.isin(batch_idus)]\n",
    "        \n",
    "            def relabel(x):\n",
    "                return x.replace('CH','CHURCH')\n",
    "\n",
    "            fix_data_list, fix_error_list = [], []\n",
    "            for v in fix_voters.iterrows():\n",
    "                #v = fix_voters.iloc[15]\n",
    "                result = cg.address(relabel(v.address), city=v.city, state=v.state, zipcode=v.zipcode)\n",
    "                if len(result)>0: \n",
    "                    result_dict = {\n",
    "                        'idu':[v.idu],\n",
    "                        'address':[relabel(v.address)],\n",
    "                        'city':[v.city],\n",
    "                        'state':[v.state],\n",
    "                        'zipcode':[v.zipcode],\n",
    "                        'parsed':[result[0]['matchedAddress']],\n",
    "                        'lat':[result[0]['coordinates']['y']],\n",
    "                        'lon':[result[0]['coordinates']['x']]\n",
    "                    }\n",
    "                    fix_data_list.append(pd.DataFrame(result_dict))\n",
    "                else:\n",
    "                    fix_error_list.append(v.idu)\n",
    "\n",
    "            fixed_batch = pd.concat(fix_data_list)\n",
    "            with open(f'{batch_path}.pkl','wb') as f:\n",
    "                pickle.dump(fixed_batch, f)\n",
    "            with open(f'{batch_path}_errors.pkl','wb') as f:\n",
    "                pickle.dump(fix_error_list, f)\n",
    "                \n",
    "            finished_fixed_batches = [batch for batch in os.listdir(path_2_batch) if '_fix.pkl' in batch]\n",
    "            if len(finished_fixed_batches)%10 == 0:\n",
    "                print_log[file]['sublog'] = [f'   || Finished {len(finished_fixed_batches)}']\n",
    "                printer(print_log)\n",
    "                \n",
    "            \n",
    "        #runtime = round(( time.time() - t0 ) / 60 )\n",
    "        #print_log[file]['log'][-1] = f'  Step 5 | Fix Errors (Runtime: {runtime} mins)'\n",
    "        #printer(print_log)\n",
    "        \n",
    "        \"\"\" Saving print_log \"\"\"\n",
    "        now = datetime.now()\n",
    "        savedate = ''.join([str(now.year),str(now.strftime('%m')),str(now.strftime('%d'))])\n",
    "        file = open(f'{path_2}{print_log}_{savedate}.txt', 'w')\n",
    "        file.write(string_printer(print_log))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_id_list = geodata.idu.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6785676"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geo_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6863676"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_819610/1265768385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfix_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo_id_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36misin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert)\u001b[0m\n\u001b[1;32m    733\u001b[0m     \"\"\"\n\u001b[1;32m    734\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m     return in1d(element, test_elements, assume_unique=assume_unique,\n\u001b[0m\u001b[1;32m    736\u001b[0m                 invert=invert).reshape(element.shape)\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36min1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mmask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#fix_list = np.isin(geo_id_list, id_list)\n",
    "# this takes awhile, so use isna instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix_list = [idu for idu in id_list if idu not in geo_id_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_id_batches = [fix_list[x:x + batch_size] for x in range(0, len(fix_list), batch_size)]\n",
    "batches = list(zip(range(len(fix_id_batches)), fix_id_batches, [print_log for b in fix_id_batches]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_errors(batch):\n",
    "    batch_num, batch_idus, print_log = batch\n",
    "    batch_path = f'{path_2_batch}{batch_num}_fix'\n",
    "    fix_voters = voters[voters.idu.isin(batch_idus)]\n",
    "\n",
    "    def relabel(x):\n",
    "        return x.replace('CH','CHURCH')\n",
    "\n",
    "    fix_data_list, fix_error_list = [], []\n",
    "    for v in fix_voters.iterrows():\n",
    "        #v = fix_voters.iloc[15]\n",
    "        result = cg.address(relabel(v.address), city=v.city, state=v.state, zipcode=v.zipcode)\n",
    "        if len(result)>0: \n",
    "            result_dict = {\n",
    "                'idu':[v.idu],\n",
    "                'address':[relabel(v.address)],\n",
    "                'city':[v.city],\n",
    "                'state':[v.state],\n",
    "                'zipcode':[v.zipcode],\n",
    "                'parsed':[result[0]['matchedAddress']],\n",
    "                'lat':[result[0]['coordinates']['y']],\n",
    "                'lon':[result[0]['coordinates']['x']]\n",
    "            }\n",
    "            fix_data_list.append(pd.DataFrame(result_dict))\n",
    "        else:\n",
    "            fix_error_list.append(v.idu)\n",
    "\n",
    "    fixed_batch = pd.concat(fix_data_list)\n",
    "    with open(f'{batch_path}.pkl','wb') as f:\n",
    "        pickle.dump(fixed_batch, f)\n",
    "    with open(f'{batch_path}_errors.pkl','wb') as f:\n",
    "        pickle.dump(fix_error_list, f)\n",
    "\n",
    "    finished_fixed_batches = [batch for batch in os.listdir(path_2_batch) if '_fix.pkl' in batch]\n",
    "    if len(finished_fixed_batches)%10 == 0:\n",
    "        print_log[file]['sublog'] = [f'   || Finished {len(finished_fixed_batches)}']\n",
    "        printer(print_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    with mp.Pool(processes = 500) as pool:\n",
    "        pool.map(fix_errors, batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "I look at:\n",
    "1. How successful the geocoding was\n",
    "2. Whether there are systematic biases in the errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
