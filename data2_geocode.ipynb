{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data 2 | Geocode Voters\n",
    "\n",
    "I look at the cleaned files and check whether they're already geocoded. I geocode them using batch_geocode() if they're not geocoded.\n",
    "\n",
    "The function batch_geocode() takes the voter dataframe, then\n",
    "1. Drops unnecessary variables\n",
    "2. Creates batches\n",
    "3. Geocodes each batch\n",
    "4. Merges the batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Key To Do\n",
    "1. Maybe do everything with the export date instead of year?\n",
    "2. Use idu as the index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data2_geocode import *\n",
    "\n",
    "\"\"\" Step 0 | Setup \"\"\"\n",
    "batch_size = 500\n",
    "\n",
    "future_files = set([x.split('_chunk_')[0] for x in os.listdir(path_1) if 'pkl' in x])\n",
    "files = [file for file in future_files if f'{file}_geo.pkl' not in os.listdir(path_2)]\n",
    "for file in sorted(files):\n",
    "    state, year = file.split('_')\n",
    "    save_file = f'{file}_geo.pkl'\n",
    "    print_log[file] = {'log':[],'sublog':[]}\n",
    "    printer(print_log)\n",
    "    \n",
    "    if save_file in os.listdir(path_2):\n",
    "        print_log[file]['log'].append('  Done')\n",
    "        printer(print_log)\n",
    "    if int(year) < 2010:\n",
    "        print_log[file]['log'].append('  Too Old (for now)')\n",
    "        printer(print_log)\n",
    "    \n",
    "    if (save_file not in os.listdir(path_2)) & (int(year) >= 2010):\n",
    "        path_2_batch = f'{path_2}{year}_batches/'\n",
    "        if not os.path.exists(path_2_batch):\n",
    "            os.makedirs(path_2_batch)\n",
    "        \n",
    "        \"\"\" Step 1 | Open Chunked Data \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 1 | Open Chunked Data')\n",
    "        printer(print_log)\n",
    "        chunk_paths, chunk_files = [f'{path_1}{chunk}' for chunk in os.listdir(path_1) if file in chunk], []\n",
    "        for chunk_path in chunk_paths:\n",
    "            chunk = pd.read_pickle(chunk_path)\n",
    "            #with open(chunk_path,'rb') as f: \n",
    "            #    chunk = pickle.load(f)\n",
    "            chunk = chunk[['idu','address','city','state','zipcode']]\n",
    "            chunk_files.append(chunk)\n",
    "        voters = pd.concat(chunk_files, ignore_index=True)\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 1 | Open Chunked Data (Runtime: {runtime} mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 2 | Create Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 2 | Create Batches')\n",
    "        printer(print_log)\n",
    "        id_list = voters.idu.unique()\n",
    "        batch_ids = [id_list[x:x + batch_size] for x in range(0, len(id_list), batch_size)]\n",
    "        batches = list(zip(range(len(batch_ids)), batch_ids, [print_log for b in batch_ids]))\n",
    "        finished_batches = os.listdir(path_2_batch)\n",
    "        run_batches, error_batches = [(b,d,printer,pl) for (b,d,pl) in batches if f'{b}.pkl' not in finished_batches], []\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 2 | Create Batches (Runtime: {runtime} mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 3 | Geocode Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append(f'  Step 3 | Geocode Batches ({len(run_batches)} of {len(batches)})')\n",
    "        printer(print_log)\n",
    "        \n",
    "        def batch_geocode(batch):\n",
    "            \"\"\" Take a list of idus and geocode them to a file. \"\"\"\n",
    "            \n",
    "            batch_num, batch_idus, printer, print_log = batch\n",
    "            batch_path = f'{path_2_batch}{batch_num}.csv'\n",
    "            send_geo = voters[voters.idu.isin(batch_idus)]\n",
    "            send_geo.to_csv(batch_path, index=False, header=False)\n",
    "            try:\n",
    "                results = cg.addressbatch(batch_path)\n",
    "                results = pd.DataFrame(results)\n",
    "                results.lat = results.lat.astype(float)\n",
    "                results.lon = results.lon.astype(float)\n",
    "                with open(batch_path.replace('.csv','.pkl'),'wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "                os.remove(batch_path)\n",
    "            except:\n",
    "                error_batches.append(batch)\n",
    "            finished_batches = [batch for batch in os.listdir(path_2_batch) if 'pkl' in batch]\n",
    "            if len(finished_batches)%10 == 0:\n",
    "                print_log[file]['sublog'] = [f'   || Finished {len(finished_batches)}']\n",
    "                printer(print_log)\n",
    "        \n",
    "        if __name__ == '__main__':\n",
    "            with mp.Pool(processes = 500) as pool:\n",
    "                pool.map(batch_geocode, run_batches)\n",
    "        \n",
    "        print_log[file]['sublog'] = ''\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 3 | Geocode Batches ({len(run_batches)} of {len(batches)}) in {runtime} mins'\n",
    "        print_log[file]['log'].append(f'    Batch Errors | {len(error_batches)}')\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 4 | Merge Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 4 | Merge Batches')\n",
    "        printer(print_log)\n",
    "        geo_paths, geodata_to_merge = [f'{path_2_batch}{batch}' for batch in os.listdir(path_2_batch) if 'pkl' in batch], []\n",
    "        for geo_path in geo_paths:\n",
    "            if os.path.getsize(geo_path) > 0:\n",
    "                with open(geo_path,'rb') as f: \n",
    "                    results = pickle.load(f)\n",
    "                    geodata_to_merge.append(results)\n",
    "        geodata = pd.concat(geodata_to_merge, ignore_index=True)\n",
    "        geodata = geodata.rename(columns = {'id':'idu'})\n",
    "        with open(f'{path_2}{file}_geo.pkl','wb') as f:\n",
    "            pickle.dump(geodata, f)\n",
    "        runtime = round(( time.time() - t0 ) / 60 )\n",
    "        print_log[file]['log'][-1] = f'  Step 4 | Merge Batches (Runtime: {runtime} mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Saving print_log \"\"\"\n",
    "        now = datetime.now()\n",
    "        savedate = ''.join([str(now.year),str(now.strftime('%m')),str(now.strftime('%d'))])\n",
    "        file = open(f'{path_2}{print_log}_{savedate}.txt', 'w')\n",
    "        file.write(string_printer(print_log))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "I look at:\n",
    "1. How successful the geocoding was\n",
    "2. Whether there are systematic biases in the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Other Methods\n",
    "\n",
    "Some voters aren't geocoded using the CBG api above. One option to increase the geocoding rate is to geocode these errors with the Google Maps and Zillow geocoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Google Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['lng','lat']+list(pa_data.columns.values)\n",
    "points = pd.read_csv('DATA0/pa.csv')\n",
    "#points = pd.DataFrame(columns = col_names)\n",
    "for row in pa_data.iterrows():\n",
    "    full_address = row[1]['House Number'] + ' ' + row[1]['Street Name'] +' ' + row[1]['City'] + ' ' + row[1]['Zip']\n",
    "    coordinates = gmaps.geocode(address=full_address)\n",
    "    if coordinates != []:\n",
    "        coordinates = gmaps.geocode(address=full_address)[0]['geometry']['location']\n",
    "        coordinates = pd.DataFrame([list(coordinates.values())+list(row[1])],columns=list(col_names))\n",
    "    else:\n",
    "        coordinates = pd.DataFrame([[np.nan,np.nan]+list(row[1])],columns=list(col_names))\n",
    "    points = points.append(coordinates)\n",
    "points = points.reset_index()\n",
    "points['Coordinates'] = list(zip(points.lng, points.lat))\n",
    "points['Coordinates'] = points['Coordinates'].apply(Point)\n",
    "#points.to_csv('DATA0/pa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Zillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data1[(data1['Last Name'] == '\"KRAYBILL\"') & (data1['First Name'] == '\"MOLLY\"')]\n",
    "address = list(x['House Number'])[0].strip('\"\"') + ' ' + list(x['Street Name'])[0].strip('\"\"')\n",
    "zipcode = list(x['Zip'])[0].strip('\"\"')\n",
    "deep_search_response=zillow_data.get_deep_search_results(address,zipcode)\n",
    "result = pyz.GetDeepSearchResults(deep_search_response)\n",
    "print(result.longitude)\n",
    "print(result.latitude)\n",
    "print(result.zestimate_valuation_range_high)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
