{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data 2 | Geocode Voters\n",
    "\n",
    "I look at the cleaned files and check whether they're already geocoded. I geocode them using batch_geocode() if they're not geocoded.\n",
    "\n",
    "The function batch_geocode() takes the voter dataframe, then\n",
    "1. Drops unnecessary variables\n",
    "2. Creates batches\n",
    "3. Geocodes each batch\n",
    "4. Merges the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NC_2021\n",
      "  Step 1 | Open Chunked Data (Runtime: 1 mins)\n",
      "  Step 2 | Create Batches (Runtime: 0 mins)\n",
      "  Step 3 | Geocode Batches (255 of 16499) in 6 mins\n",
      "    Batch Errors | 33\n",
      "  Step 4 | Merge Batches (Runtime: 0.8166666666666667 mins)\n"
     ]
    }
   ],
   "source": [
    "from data2_geocode import *\n",
    "\n",
    "\"\"\" Step 0 | Setup \"\"\"\n",
    "batch_size = 500\n",
    "\n",
    "future_files = set([x.split('_chunk_')[0] for x in os.listdir(path_1) if x.split('.')[-1] == 'pkl'])\n",
    "files = [x for x in future_files if x+'_geo.pkl' not in os.listdir(path_2)]\n",
    "for file in sorted(files):\n",
    "    state, year = file.split('_')\n",
    "    print_log[file] = {'log':[],'sublog':[]}\n",
    "    printer(print_log)\n",
    "    \n",
    "    if file + '_geo.pkl' in os.listdir(path_2):\n",
    "        print_log[file]['log'].append('  Done')\n",
    "        printer(print_log)\n",
    "    \n",
    "    if int(year) < 2010:\n",
    "        print_log[file]['log'].append('  Too Old (for now)')\n",
    "        printer(print_log)\n",
    "        \n",
    "    if (file + '_geo.pkl' not in os.listdir(path_2)) & (int(year) >= 2010):\n",
    "        path_2_batch = path_2+str(year)+'_batches/'\n",
    "        if not os.path.exists(path_2_batch):\n",
    "            os.makedirs(path_2_batch)\n",
    "        \n",
    "        \"\"\" Step 1 | Open Chunked Data \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 1 | Open Chunked Data')\n",
    "        printer(print_log)\n",
    "        chunk_file_names, chunk_files = [x for x in os.listdir(path_1) if x.split('_chunk_')[0] == file], []\n",
    "        for chunk_file_name in chunk_file_names:\n",
    "            with open(path_1 + chunk_file_name,'rb') as f: \n",
    "                chunk = pickle.load(f)\n",
    "                chunk = chunk[['idu','address','city','state','zipcode']]\n",
    "            chunk_files.append(chunk)\n",
    "        voters = pd.concat(chunk_files, ignore_index=True)\n",
    "        runtime = str(round(( time.time() - t0 ) / 60 ))\n",
    "        print_log[file]['log'][-1] = '  Step 1 | Open Chunked Data (Runtime: ' + runtime + ' mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 2 | Create Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 2 | Create Batches')\n",
    "        printer(print_log)\n",
    "        id_list = voters.idu.unique()\n",
    "        batch_ids = [id_list[x:x+batch_size] for x in range(0, len(id_list), batch_size)]\n",
    "        batches = list(zip(range(len(batch_ids)), batch_ids, [print_log for b in batch_ids]))\n",
    "        finished_batches = os.listdir(path_2_batch)\n",
    "        run_batches, error_batches = [(b,d,printer,pl) for (b,d,pl) in batches if str(b)+'.pkl' not in finished_batches], []\n",
    "        runtime = str(round(( time.time() - t0 ) / 60 ))\n",
    "        print_log[file]['log'][-1] = '  Step 2 | Create Batches (Runtime: ' + runtime + ' mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 3 | Geocode Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 3 | Geocode Batches (' + str(len(run_batches)) + ' of ' + str(len(batches))+')')\n",
    "        printer(print_log)\n",
    "        \n",
    "        def batch_geocode(batch):\n",
    "            \"\"\" Take a list of idus and geocode them to a file. \"\"\"\n",
    "            \n",
    "            batch_num, batch_idus, printer, print_log = batch\n",
    "            batch_path = path_2_batch+str(batch_num)\n",
    "            send_geo = voters[voters.idu.isin(batch_idus)]\n",
    "            send_geo.to_csv(batch_path+'.csv', index=False, header=False)\n",
    "            try:\n",
    "                results = cg.addressbatch(batch_path+'.csv')\n",
    "                results = pd.DataFrame(results)\n",
    "                results.lat = results.lat.astype(float)\n",
    "                results.lon = results.lon.astype(float)\n",
    "                with open(batch_path+'.pkl','wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "                os.remove(batch_path+'.csv')\n",
    "            except:\n",
    "                error_batches.append(batch)\n",
    "            finished_batches = [x for x in os.listdir(path_2_batch) if x.split('.')[-1] == 'pkl']\n",
    "            if len(finished_batches)%10 == 0:\n",
    "                print_log[file]['sublog'] = ['   || Finished ' + str(len(finished_batches))]\n",
    "                printer(print_log)\n",
    "        \n",
    "        if __name__ == '__main__':\n",
    "            with mp.Pool(processes = 500) as pool:\n",
    "                pool.map(batch_geocode, run_batches)\n",
    "        \n",
    "        print_log[file]['sublog'] = ''\n",
    "        runtime = str(round(( time.time() - t0 ) / 60 ))\n",
    "        print_log[file]['log'][-1] = '  Step 3 | Geocode Batches (' + str(len(run_batches)) + ' of ' + str(len(batches))+') in ' + runtime + ' mins'\n",
    "        print_log[file]['log'].append('    Batch Errors | ' + str(len(error_batches)))\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Step 4 | Merge Batches \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[file]['log'].append('  Step 4 | Merge Batches')\n",
    "        printer(print_log)\n",
    "        geofiles_to_load, geodata_to_merge = [x for x in os.listdir(path_2_batch) if x.split('.')[-1] == 'pkl'], []\n",
    "        for geofile in geofiles_to_load:\n",
    "            if os.path.getsize(path_2_batch + geofile) > 0:\n",
    "                with open(path_2_batch + geofile,'rb') as f: \n",
    "                    results = pickle.load(f)\n",
    "                    geodata_to_merge.append(results)\n",
    "        geodata = pd.concat(geodata_to_merge, ignore_index=True)\n",
    "        geodata = geodata.rename(columns = {'id':'idu'})\n",
    "        with open(path_2 + file + '_geo.pkl','wb') as f:\n",
    "            pickle.dump(geodata, f)\n",
    "        runtime = str(round(time.time()-t0,0)/60)\n",
    "        print_log[file]['log'][-1] = '  Step 4 | Merge Batches (Runtime: ' + runtime + ' mins)'\n",
    "        printer(print_log)\n",
    "        \n",
    "        \"\"\" Saving print_log \"\"\"\n",
    "        now = datetime.now()\n",
    "        savedate = ''.join([str(now.year),str(now.strftime('%m')),str(now.strftime('%d'))])#,str(now.strftime('%H'))])\n",
    "        file = open(path_3 + 'print_log_' + savedate + '.txt', 'w')\n",
    "        file.write(string_printer(print_log))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "I look at:\n",
    "1. How successful the geocoding was\n",
    "2. Whether there are systematic biases in the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Other Methods\n",
    "\n",
    "Some voters aren't geocoded using the CBG api above. One option to increase the geocoding rate is to geocode these errors with the Google Maps and Zillow geocoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Google Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['lng','lat']+list(pa_data.columns.values)\n",
    "points = pd.read_csv('DATA0/pa.csv')\n",
    "#points = pd.DataFrame(columns = col_names)\n",
    "for row in pa_data.iterrows():\n",
    "    full_address = row[1]['House Number'] + ' ' + row[1]['Street Name'] +' ' + row[1]['City'] + ' ' + row[1]['Zip']\n",
    "    coordinates = gmaps.geocode(address=full_address)\n",
    "    if coordinates != []:\n",
    "        coordinates = gmaps.geocode(address=full_address)[0]['geometry']['location']\n",
    "        coordinates = pd.DataFrame([list(coordinates.values())+list(row[1])],columns=list(col_names))\n",
    "    else:\n",
    "        coordinates = pd.DataFrame([[np.nan,np.nan]+list(row[1])],columns=list(col_names))\n",
    "    points = points.append(coordinates)\n",
    "points = points.reset_index()\n",
    "points['Coordinates'] = list(zip(points.lng, points.lat))\n",
    "points['Coordinates'] = points['Coordinates'].apply(Point)\n",
    "#points.to_csv('DATA0/pa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Zillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data1[(data1['Last Name'] == '\"KRAYBILL\"') & (data1['First Name'] == '\"MOLLY\"')]\n",
    "address = list(x['House Number'])[0].strip('\"\"') + ' ' + list(x['Street Name'])[0].strip('\"\"')\n",
    "zipcode = list(x['Zip'])[0].strip('\"\"')\n",
    "deep_search_response=zillow_data.get_deep_search_results(address,zipcode)\n",
    "result = pyz.GetDeepSearchResults(deep_search_response)\n",
    "print(result.longitude)\n",
    "print(result.latitude)\n",
    "print(result.zestimate_valuation_range_high)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
