{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 5 | Supplemental Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data5_supplemental import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Phone Numbers and Semi-Automated Texting\n",
    "\n",
    "To do anything involving contacting voters, I first need IRB approval since it's research involving human subjects.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Area_codes_336_and_743#/media/File:USA_telephone_area_code_map_-_North_Carolina.svg\n",
    "\n",
    "And to legally contact voters I need to send the text message manually. Auto-texters are banned. But according to the fcc:\n",
    "\n",
    "\"political text messages can be sent without the intended recipient’s prior consent if the message’s sender does not use autodialing technology to send such texts and instead manually dials them\"\n",
    "\n",
    "https://www.fcc.gov/rules-political-campaign-calls-and-texts\n",
    "\n",
    "After doing some reading, it sounds as if call-center type software is allowed, but the opporator is required to click send. One source claims they can send up to 800 texts per hour, which is 13 texts per minute. Not sure how much software is involved ... or is allowed to be involved.\n",
    "\n",
    "https://stonesphones.com/blog/voter-file-texting-what-you-should-know/\n",
    "\n",
    "Here's a document with more specific language:\n",
    "\n",
    "https://bolderadvocacy.org/wp-content/uploads/2016/02/AFJ_Rules-of-Robo_web2-1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the map of phone area codes\n",
    "\n",
    "https://en.wikipedia.org/wiki/Area_codes_336_and_743#/media/File:USA_telephone_area_code_map_-_North_Carolina.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Interpolation using Squares\n",
    "\n",
    "This used to be part of data3. I've simply coppied the code from data3 here and into the import file without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meterlist = [50,75,100,150,250,500,1000,2000]\n",
    "folder,state,meters,print_log = 'data3_neighborhoods/','NC',meterlist[5],{}\n",
    "path = 'data3_neighborhoods/'+state+'/'\n",
    "\n",
    "meta_yearlist = [2005,2006,2007,2008,2009,2010,2011,2012,2013,2015,2016,2017,2018,2019][::-1]\n",
    "yearlist = meta_yearlist\n",
    "\n",
    "#nc_acs_summary = return_nc_acs_summary()\n",
    "\n",
    "\"\"\" Step 0 | Setup \"\"\"\n",
    "data_name = state+' @ '+str(meters)+'m'\n",
    "print_log[data_name] = {'log':[]}\n",
    "\n",
    "if 'Ad_'+str(meters)+'m.pkl' in os.listdir(folder+state+'/'):\n",
    "    print_log[data_name]['log'] = ['  Step 0 | Load Grid']\n",
    "    printer(print_log)\n",
    "    with open(folder+state+'/WHd_'+str(meters)+'m.pkl','rb') as f: WHd = pickle.load(f)\n",
    "    with open(folder+state+'/Ad_'+str(meters)+'m.pkl','rb') as f: Ad = pickle.load(f)\n",
    "    with open(folder+state+'/Cd_'+str(meters)+'m.pkl','rb') as f: Cd = pickle.load(f)\n",
    "else:\n",
    "    print_log[data_name]['log'] = ['  Step 0 | Generate Grid']\n",
    "    printer(print_log)\n",
    "    data = clean_data(state,2010)\n",
    "    W,H = define_squares(data,meters)\n",
    "    Ad,WHd,Cd = adjacency_matrix(W,H,data_name,print_log,path)\n",
    "    with open(folder+state+'/WHd_'+str(meters)+'m.pkl','wb') as f: pickle.dump(WHd, f)\n",
    "    with open(folder+state+'/Ad_'+str(meters)+'m.pkl','wb') as f: pickle.dump(Ad, f)\n",
    "    with open(folder+state+'/Cd_'+str(meters)+'m.pkl','wb') as f: pickle.dump(Cd, f)\n",
    "\n",
    "for year in yearlist:\n",
    "    T0 = time.time()\n",
    "    data_name = str(year)#state+' '+str(year)+' - '+str(meters)+'m'\n",
    "    print_log[data_name] = {'log':[],'sublog':[]}\n",
    "    printer(print_log)\n",
    "    \n",
    "    if 'voters_'+str(year)+'_'+str(meters)+'m.csv' in os.listdir(path):\n",
    "        print_log[data_name]['log'].append('  Done')\n",
    "        printer(print_log)\n",
    "        \n",
    "    if 'voters_'+str(year)+'_'+str(meters)+'m.csv' not in os.listdir(path):\n",
    "        \n",
    "        \"\"\" Step 1 | Open and Clean \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[data_name]['log'].append('  Step 1 | Open and Clean')\n",
    "        printer(print_log)\n",
    "        data = clean_data(state,year)\n",
    "        print_log[data_name]['log'][-1] = '  Step 1 | Open and Clean (Runtime: '+str(round((time.time()-t0)/60))+' mins)'\n",
    "        \n",
    "        \"\"\" Step 2 | Populate Squares \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[data_name]['log'].append('  Step 2 | Populating Squares')   \n",
    "        SNd,POPd = populate_squares(data,WHd,print_log,data_name,meters)\n",
    "        print_log[data_name]['log'][-1] = '  Step 2 | Populating Squares (Runtime: '+str(round((time.time()-t0)/60))+' mins; Squares: '+str(len(SNd))+')'\n",
    "        del data\n",
    "        \n",
    "        \"\"\" Step 3 | Spatial Interpolation \"\"\"\n",
    "        \"\"\"\n",
    "        t0 = time.time()\n",
    "        print_log[data_name]['log'].append('  Step 3 | Spatial Interpolation')\n",
    "        SNd_int = spatial_interpolate(SNd,nc_acs_summary,data_name,print_log,year)\n",
    "        print_log[data_name]['log'][-1] = '  Step 3 | Spatial Interpolation (Runtime: '+str(round((time.time()-t0)/60))+')'\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" Step -1 | Merge and Save \"\"\"\n",
    "        print_log[data_name]['log'].append('  Step -1 | Merge and Save')\n",
    "        with open('data3_neighborhoods/'+state+'/SNd_'+str(year)+'_'+str(meters)+'m.pkl','wb') as f: pickle.dump(SNd, f)\n",
    "        with open('data3_neighborhoods/'+state+'/POPd_'+str(year)+'_'+str(meters)+'m.pkl','wb') as f: pickle.dump(POPd, f)\n",
    "        SNfinal = [SNd[i] for i in SNd]\n",
    "        SNfinal = pd.concat(SNfinal,ignore_index=True, sort=False)\n",
    "        SNfinal.to_csv('data3_neighborhoods/'+state+'/voters_'+str(year)+'_'+str(meters)+'m.csv')\n",
    "        print_log[data_name]['log'][-1] = '  Step -1 | Merge and Save (Total Runtime: '+str(round((time.time()-T0)/60))+' mins)'\n",
    "        printer(print_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geography and Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_population = pd.read_csv('data0_raw/nhgis/county_population/nhgis0001_county.csv', encoding='latin-1')\n",
    "col_rename = {x:'POP_'+x.replace('A00AA','') for x in county_population.columns.values[7:]}\n",
    "col_rename_names = [col_rename[x] for x in col_rename if int(col_rename[x].replace('POP_','')) >= 1920]\n",
    "keep_cols = ['GISJOIN','STATE','COUNTY','COUNTYNH'] + col_rename_names\n",
    "county_population = county_population.rename(columns=col_rename)\n",
    "\n",
    "county_shapefile = gpd.read_file('data0_raw/nhgis/county_shape/us_county_2000/US_county_2000.shp')\n",
    "keep_cols = ['NHGISNAM', 'ICPSRCTY', 'STATENAM', 'GISJOIN', 'SHAPE_AREA', 'geometry'] + col_rename_names\n",
    "counties = county_population.merge(county_shapefile, on='GISJOIN')[keep_cols].reset_index(drop=True)\n",
    "\n",
    "for col in [col_rename[x] for x in col_rename if int(col_rename[x].replace('POP_','')) >= 1920]:\n",
    "    counties['DENS_'+col.replace('POP_','')] = counties[col]/counties['SHAPE_AREA']\n",
    "nc_counties = counties[counties.STATENAM == 'North Carolina'].reset_index(drop=True)\n",
    "gpd.GeoDataFrame(counties).to_file('data6_supplemental/counties.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrape County Voting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/United_States_presidential_elections_in_Washington,_D.C.\n",
    "# This doesn't work perfectly. There are many years for many states that aren't getting collected despite being present.\n",
    "# It might be worth doing this individually by state-year for specific states.\n",
    "\n",
    "#state_names_alt = [\"Alaska\"]\n",
    "yearlist = np.arange(1940,2020,4)\n",
    "state_elections = {}\n",
    "\n",
    "for state in state_names:\n",
    "    state = state.replace(' ','_')\n",
    "    state_elections[state] = {}\n",
    "    for year in yearlist:\n",
    "        url = \"https://en.wikipedia.org/wiki/\"+str(year)+\"_United_States_presidential_election_in_\"+state\n",
    "        try:\n",
    "            table_list = pd.read_html(url)\n",
    "            clean_up = False\n",
    "            for table in table_list:\n",
    "                col_names = list(table.columns.values)\n",
    "                if type(col_names[0]) == str:\n",
    "                    if ('County' in col_names[0]) or ('Parish' in col_names[0]):\n",
    "                        table['state'] = state\n",
    "                        state_elections[state][year] = table\n",
    "                        clean_up = True\n",
    "                if type(col_names[0]) == tuple:\n",
    "                    for subcol_name in col_names[0]:\n",
    "                        if ('County' in subcol_name) or ('Parish' in subcol_name):\n",
    "                            table['state'] = state\n",
    "                            state_elections[state][year] = table\n",
    "                            clean_up = True\n",
    "                            \n",
    "            if clean_up == True:\n",
    "                table = state_elections[state][year]\n",
    "                col_names = table.columns.values\n",
    "                selected_cols = [table.columns.values[0], table.columns.values[-1]]\n",
    "                for col in col_names:\n",
    "                    if type(col) == tuple:\n",
    "                        for subcol in col:\n",
    "                            #if '%' in table[col].iloc[0]:\n",
    "                            #    selected_cols.append(col)\n",
    "                            if '%' in subcol:\n",
    "                                selected_cols.append(col)\n",
    "                            if 'Percentage' in subcol:\n",
    "                                selected_cols.append(col)\n",
    "                            if 'percentage' in subcol:\n",
    "                                selected_cols.append(col)\n",
    "                    if type(col) == str:\n",
    "                        if '%' in col:\n",
    "                            selected_cols.append(col)\n",
    "                        if 'Percentage' in col:\n",
    "                            selected_cols.append(col)\n",
    "                        if 'percentage' in col:\n",
    "                            selected_cols.append(col)\n",
    "                table = table[selected_cols]\n",
    "                table.columns = table.columns.map(''.join)\n",
    "                rename_dict = {col:rename_columns(col,year) for col in table.columns}\n",
    "                table = table.rename(columns = rename_dict)\n",
    "\n",
    "                index_names = table[table['County'].isin(['Totals','Total','All '])].index\n",
    "                table.drop(index_names, inplace=True)\n",
    "\n",
    "                table = table.sort_values(by='County', ignore_index=True)\n",
    "                table = table[[rename_dict[x] for x in rename_dict if rename_dict[x] != 'O']]\n",
    "                for col in [x for x in table.columns.values if (x != 'County') & (x != 'state')]:\n",
    "                    table[col] = [float(str(x).strip('%w').replace('Source: [20]', 'nan')) for x in table[col]]\n",
    "                table['State_County'] = table['state'] +'|'+ table['County']\n",
    "\n",
    "                state_elections[state][year] = table\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virginia\n",
      "sub Virginia\n"
     ]
    }
   ],
   "source": [
    "datalist,datadict = {},[]\n",
    "for state in state_elections:\n",
    "    try:\n",
    "        state_datalist = [state_elections[state][year].set_index('County') for year in state_elections[state] if 'D_'+str(year) in state_elections[state][year].columns.values] #.set_index('State_County')[['D_'+str(year),'R_'+str(year)]]\n",
    "        datalist.append(state_datalist)\n",
    "        print(state)\n",
    "    except:\n",
    "        try:\n",
    "            state_datalist = [state_elections[state][year].set_index('Parish') for year in state_elections[state] if 'D_'+str(year) in state_elections[state][year].columns.values] #.set_index('State_County')[['D_'+str(year),'R_'+str(year)]]\n",
    "            datalist.append(state_datalist)\n",
    "        except:\n",
    "            print(state)\n",
    "    try:\n",
    "        concat_list = pd.concat(state_datalist,axis=1)\n",
    "        concat_list.to_csv('data6_supplemental/state_vote_share/'+state+'.csv')\n",
    "        datadict[state] = concat_list\n",
    "    except:\n",
    "        print('sub '+state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = [elections[year].set_index('State_County') for year in elections]\n",
    "election_hist = pd.concat(datalist, axis=1)\n",
    "index_names = list({x for x in election_hist.index if ('All ' in x) or ('Total' in x)})\n",
    "election_hist.drop(index_names, inplace=True)\n",
    "election_hist['State_County'] = election_hist.index\n",
    "election_hist = election_hist.reset_index(drop=True)\n",
    "election_hist.to_csv('data6_supplemental/election_hist_raw.csv')\n",
    "\n",
    "election_hist['State_County'] = [x.replace('_',' ') for x in election_hist['State_County']]\n",
    "election_hist = election_hist.drop(columns=['County','state'])\n",
    "election_hist['state'] = [x.split('|')[0] for x in election_hist['State_County'] if len(x.split('|'))==2]\n",
    "election_hist['County'] = [x.split('|')[1] for x in election_hist['State_County'] if len(x.split('|'))==2]\n",
    "election_hist.to_csv('data6_supplemental/election_hist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vote Share and Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties['STATENAM'] = [x.replace('_',' ') for x in counties['STATENAM']]\n",
    "counties['State_County'] = counties['STATENAM'] +'|'+ counties['NHGISNAM']\n",
    "\n",
    "vote_share_density = counties.merge(election_hist, on='State_County')\n",
    "vote_share_density.to_csv('electoral_divergence_d/vote_share_density.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census & ACS PUMAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = pd.read_csv('data0_raw/census/census_1980_2000.csv')\n",
    "census = census.dropna(subset=['puma'])\n",
    "census.puma = census.puma.astype(int)\n",
    "nc_census = census[census.statefip=='North Carolina']\n",
    "\n",
    "acs = pd.read_csv('data0_raw/census/acs_2004_2019.csv')\n",
    "acs = acs.dropna(subset=['puma'])\n",
    "acs.puma = acs.puma.astype(int)\n",
    "nc_acs = acs[acs.statefip=='North Carolina']\n",
    "\n",
    "education = ['College','Some College','No College']\n",
    "\n",
    "yearlist_census = list(nc_census.year.unique())\n",
    "yearlist_acs = [x for x in nc_acs.year.unique() if x >=2005]\n",
    "yeardict_census_acs = {year:np.nan for year in yearlist_census + yearlist_acs}\n",
    "\n",
    "puma_yeardict = {year:year for year in yearlist_census + yearlist_acs}\n",
    "for year in nc_puma_yearmap:\n",
    "    if year in yearlist_census:\n",
    "        puma_year = gpd.read_file('data0_raw/shapefiles/ipums_puma_'+str(year)+'/ipums_puma_'+str(year)+'.shp')\n",
    "    if year < 2012:\n",
    "        puma_year = gpd.read_file('data0_raw/shapefiles/ipums_puma_'+str(2000)+'/ipums_puma_'+str(2000)+'.shp')\n",
    "    if year >= 2012:\n",
    "        puma_year = gpd.read_file('data0_raw/shapefiles/ipums_puma_'+str(2010)+'/ipums_puma_'+str(2010)+'.shp')\n",
    "        \n",
    "    nc_puma_year = puma_year[puma_year.STATEFIP == '37']\n",
    "    nc_puma_year.PUMA = nc_puma_year.PUMA.astype(int)\n",
    "    puma_yeardict[year] = nc_puma_year\n",
    "\n",
    "for year in yearlist_census + yearlist_acs:\n",
    "    if year in yearlist_census:\n",
    "        nc_year = nc_census[(nc_census.year == year)]\n",
    "    if year in yearlist_acs:\n",
    "        nc_year = nc_acs[(nc_acs.year == year)]\n",
    "    \n",
    "    nc_puma_list = [x for x in nc_year.puma.unique() if x >= 0]\n",
    "    nc_puma = puma_yeardict[year]\n",
    "    selected_pumas = nc_puma[nc_puma.PUMA.isin(nc_puma_list)]\n",
    "    \n",
    "    average_data = []\n",
    "    for puma in nc_puma_list:\n",
    "        averages,col_names = [puma],['puma']\n",
    "        \n",
    "        subdata = nc_year[(nc_year.puma == puma)]\n",
    "        population = sum(subdata.perwt)\n",
    "        white_pop = sum(subdata[subdata.race == 'White'].perwt)\n",
    "        black_pop = sum(subdata[subdata.race == 'Black'].perwt)\n",
    "        averages = averages + [population, white_pop, black_pop]\n",
    "        col_names = col_names + ['POP','WHITE','BLACK']\n",
    "        \n",
    "        subdata = subdata[(subdata.age >= 18)]\n",
    "        averages = averages + [sum(subdata.perwt)]\n",
    "        col_names = col_names + ['VOTING_POP']\n",
    "        \n",
    "        subdata = subdata[(subdata.empstat != 'Not in labor force')]\n",
    "        averages = averages + [sum(subdata.perwt)]\n",
    "        col_names = col_names + ['LABOR']\n",
    "        \n",
    "        averages = averages + [sum(subdata.age*subdata.perwt)/sum(subdata.perwt), sum(subdata.trantime*subdata.perwt)/sum(subdata.perwt)]\n",
    "        col_names = col_names + ['AGE', 'TRANTIME']\n",
    "        \n",
    "        for ed_type in education:\n",
    "            ed_type_data = subdata[subdata.educ == ed_type]\n",
    "            ed_type = ed_type.upper().replace(' ','_')\n",
    "            averages = averages + [sum(ed_type_data.perwt)]\n",
    "            col_names = col_names + [ed_type]\n",
    "            \n",
    "            ed_type_data = ed_type_data[ed_type_data.empstat == 'Employed']\n",
    "            averages = averages + [sum(ed_type_data.perwt), sum(ed_type_data.incwage*ed_type_data.perwt)/sum(ed_type_data.perwt)]\n",
    "            col_names = col_names + [ed_type+'_EMPLOYED', ed_type+'_WAGE']\n",
    "\n",
    "        average_data.append(averages)\n",
    "    average_data = pd.DataFrame(average_data, columns=col_names)\n",
    "    \n",
    "    selected_pumas = nc_puma[nc_puma.PUMA.isin(nc_puma_list)]\n",
    "    selected_data = average_data[average_data.puma.isin(selected_pumas.PUMA.unique())]\n",
    "    \n",
    "    nc_puma_data = pd.concat([selected_pumas.sort_values(by='PUMA').reset_index(drop=True), selected_data.sort_values(by='puma').reset_index(drop=True)], axis=1)\n",
    "\n",
    "    nc_puma_data['AREA'] = nc_puma_data.area\n",
    "    nc_puma_data['DENSITY'] = nc_puma_data['POP']/nc_puma_data['AREA']\n",
    "    \n",
    "    yeardict_census_acs[year] = nc_puma_data\n",
    "with open('data6_supplemental/yeardict_census_acs.pkl','wb') as f: pickle.dump(yeardict_census_acs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
