{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data 1 | Clean Voterfiles\n",
    "\n",
    "First I unzip the text files, open the text files as dataframe, the delete the textfile. Then I clean the dataframe:\n",
    "1. Generate new variables\n",
    "2. Rename necessary variables\n",
    "3. Drop unnecessary variables\n",
    "4. Reformat the necessary variables\n",
    "5. Then generate new variables using the reformatted variables\n",
    "\n",
    "All data retrieved from https://dl.ncsbe.gov/?prefix=data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge Turnout Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalink import *\n",
    "\n",
    "hists = [x for x in os.listdir(path_0 + 'history') if 'ncvhis_Statewide' in x and '.txt' in x]\n",
    "hists = [pd.read_csv(path_0 + 'history/'+x, sep='\\t', header=0) for x in hists]\n",
    "print('Loaded History Files')\n",
    "\n",
    "hist = pd.concat(hists, ignore_index=True)\n",
    "del hists\n",
    "print('Concat Finished')\n",
    "\n",
    "hist = hist.drop_duplicates()\n",
    "print('Dropped Duplicates')\n",
    "\n",
    "hist.to_pickle(path_0 + 'history/turnout_history.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Clean Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data1_clean import *\n",
    "\n",
    "history = pd.read_pickle(f'{path_0}/history/turnout_history.pkl')\n",
    "\n",
    "for date in dates:\n",
    "    print(date)\n",
    "    file, year = zip_map[date], year_map[date]\n",
    "    \n",
    "    if f'{date}_chunk_0.pkl' in os.listdir(path_1):\n",
    "        print('  Done')\n",
    "        \n",
    "    if int(year) < 2010:\n",
    "        print('  Too Old (for now)')\n",
    "        \n",
    "    if (f'{date}_chunk_0.pkl' not in os.listdir(path_1)) and (int(year) >= 2010):\n",
    "        \n",
    "        \"\"\" Unzip Snapshot \"\"\"\n",
    "        \n",
    "        os.system(f'unzip -o {path_0_zip}{file} -d {path_0_zip}')\n",
    "        zipfile = ZipFile(f'{path_0_zip}{file}')\n",
    "        extracted_file, = zipfile.namelist()\n",
    "\n",
    "        \"\"\" Chunk and Clean Snapshot \"\"\"\n",
    "\n",
    "        voter_chunks = pd.read_csv(path_0_zip + extracted_file, \n",
    "                                    low_memory=False, \n",
    "                                    iterator=True, \n",
    "                                    chunksize=10**6, \n",
    "                                    sep='\\t', \n",
    "                                    encoding_errors='ignore', \n",
    "                                    error_bad_lines=False, \n",
    "                                    quoting=csv.QUOTE_NONE)\n",
    "                                    \n",
    "        for index, voters in enumerate(voter_chunks):\n",
    "            print(f'  Cleaning Chunk {index}')\n",
    "            voters = clean_NC(voters, history, date, nc_rename, run_elections=True)\n",
    "            print(f'  Saving Chunk {index}')\n",
    "\n",
    "            voters.to_pickle(f'{path_1}{date}_chunk_{index}.pkl')\n",
    "        \n",
    "        os.remove(path_0_zip + extracted_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"\n",
    "        try:\n",
    "            voter_chunks = pd.read_csv(path_0_zip + extracted_file, \n",
    "                                       low_memory=False, \n",
    "                                       iterator=True, \n",
    "                                       chunksize=10**6, \n",
    "                                       sep='\\t', \n",
    "                                       encoding='utf-16', \n",
    "                                       encoding_errors='ignore', \n",
    "                                       error_bad_lines=False, \n",
    "                                       quoting=csv.QUOTE_NONE)\n",
    "        except:\n",
    "        print('Not encoding utf-16')\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trusty Old Version\n",
    "\n",
    "This code works with previously extracted and converted csv files. The new version extracts the snapshot zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VR_Snapshot_20051125.csv\n",
      "  Done\n",
      "VR_Snapshot_20060210.csv\n",
      "  Done\n",
      "VR_Snapshot_20070119.csv\n",
      "  Done\n",
      "VR_Snapshot_20081104.csv\n",
      "  Done\n",
      "VR_Snapshot_20090101.csv\n",
      "  Done\n",
      "VR_Snapshot_20100101.csv\n",
      "  Done\n",
      "VR_Snapshot_20110101.csv\n",
      "  Done\n",
      "VR_Snapshot_20120101.csv\n",
      "  Done\n",
      "VR_Snapshot_20130101.csv\n",
      "  Done\n",
      "VR_Snapshot_20150101.csv\n",
      "  Done\n",
      "VR_Snapshot_20160101.csv\n",
      "  Done\n",
      "VR_Snapshot_20161108.csv\n",
      "  Done\n",
      "VR_Snapshot_20170101.csv\n",
      "  Done\n",
      "VR_Snapshot_20180101.csv\n",
      "  Done\n",
      "VR_Snapshot_20190101.csv\n",
      "  Done\n"
     ]
    }
   ],
   "source": [
    "from data1_clean import *\n",
    "\n",
    "with open(path_0 + 'turnout_history_20190101.pkl','rb') as f: \n",
    "    history = pickle.load(f)\n",
    "with open(path_0 + 'history_idus_20190101.pkl','rb') as f: \n",
    "    history_idus = pickle.load(f)\n",
    "\n",
    "snapshot_filelist = sorted([x for x in os.listdir(path_0_csv) if x.split('.')[-1] == 'csv'])\n",
    "for file in snapshot_filelist:\n",
    "    print(file)\n",
    "    \n",
    "    if 'NC_' + year + '_chunk_0.pkl' in os.listdir(path_1):\n",
    "        print('  Done')\n",
    "        \n",
    "    else:\n",
    "        year = file.split('_')[-1][:4]\n",
    "\n",
    "        voter_chunks, voter_list, chunk_index = pd.read_csv(path_0_csv + file, dtype='str', low_memory=False, iterator=True, chunksize=10**6), [], -1\n",
    "        for voters in voter_chunks:\n",
    "\n",
    "            print('  Cleaning Chunk ' + str(chunk_index))# + ' (of '+str(len(voter_chunks))+')')\n",
    "            voters, chunk_index = clean_NC(voters, history, history_idus, year, nc_rename), chunk_index + 1\n",
    "            print('  Saving Chunk ' + str(chunk_index))# + ' (of '+str(len(voter_chunks))+')')\n",
    "\n",
    "            with open(path_1 + '/NC_' + year + '_chunk_' + str(chunk_index) + '.pkl','wb') as f:\n",
    "                pickle.dump(voters, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
